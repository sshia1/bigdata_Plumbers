    1  cd
    2  sudo apt update
    3  sudo apt-get update
    4  reboot
    5  cd ..
    6  cd Downloads/
    7  ls
    8  tar xvfz ideaIC-2020.1.2.tar.gz 
    9  exit
   10  apt update
   11  apt upgrade
   12  sudo apt update
   13  sudo apt upgrade
   14  exit
   15  ls
   16  cd Downloads
   17  ls
   18  sudo tar -xzf ideaIC-2020.1.2.tar.gz -C /opt
   19  cd opt
   20  ls
   21  ls -l
   22  sudo tar -xzf ideaIC-2020.1.2.tar.gz -C /opt
   23  ls
   24  exit
   25  ls
   26  cd downloads
   27  cd Downloads
   28  ls
   29  cd ideaIC-2020.1.2.tar.gz
   30  ls -l
   31  cd ideaID-2020.1.2
   32  cd idea-IC-201.7846.76/
   33  ls
   34  cd bin
   35  ls
   36  ls /w
   37  ls -l
   38  idea.sh
   39  .\idea.sh
   40  ./idea.sh
   41  cd Downloads
   42  ls -l
   43  sudo tar -xzf jdk-8u251-linux-i586.tar.gz  -C /opt
   44  ls -w
   45  ls --help
   46  ls -w
   47  ls --help
   48  ls -l
   49  sudo tar -xzf jdk-8u251-linux-i586.tar.gz  -C /opt
   50  ls -l
   51  cd
   52  ls
   53  mkdir opt
   54  ls -l
   55  cd Downloads
   56  ls -l
   57  mv jdk-8u251-linux-i586.tar.gz ../opt
   58  cd ../opt
   59  ls -l
   60  sudo tar -xzf jdk-8u251-linux-i586.tar.gz  
   61  ls -l
   62  cd jdk1.8.0_251/
   63  ls -l
   64  python --version
   65  python -version
   66  python3 -version
   67  python3
   68  python3 -version
   69  python3 -h
   70  python3 -V
   71  java -V
   72  cd ..
   73  cd Downloads
   74  ls
   75  installsbt
   76  installsbt.sh
   77  shell installsbt.sh
   78  type installsbt.sh
   79  cat installsbt.sh
   80  installsbt.sh
   81  ./installsbt.sh
   82  chmod +x installsbt.sh
   83  ./installsbt.sh
   84  ls
   85  echo $ORACLE_HOME
   86  date
   87  TZ=GMT date
   88  date
   89  date -d now
   90  date
   91  date -d yesterday
   92  date
   93  date -d tomorrow
   94  date
   95  date -d now
   96  date
   97  df
   98  df -h
   99  whatis whatis
  100  whatis df
  101  free
  102  whatis free
  103  ps
  104  whatis ps
  105  uptime
  106  whatis uptime
  107  w
  108  whatis w
  109  w
  110  password
  111  whatis password
  112  passwd
  113  whatis exit
  114  exit
  115  whatis shutdown
  116  mkdir
  117  mdir --help
  118  mkdir --help | more
  119  whatis more
  120  rmdir
  121  rmdir --help
  122  rm
  123  rm --help
  124  whatis rm
  125  mv
  126  mv --help
  127  whatis mv
  128  cp
  129  cp --help
  130  whatis cp
  131  cat
  132  clear
  133  echo "t1" > t1.txt
  134  cat t1.txt
  135  echo "t2" > t2.txt
  136  cat t2.txt
  137  cat t1.txt t2.txt
  138  echo "1 2 3" t3.txt
  139  cat t3.txt
  140  echo "1 2 3" > t3.txt
  141  cat t3.txt
  142  tac t3.txt
  143  cat t1.txt t2.txt
  144  tac t1.txt t2.txt
  145  tac t1.txt t2.txt t3.txt
  146  cat t1.txt t2.txt t3.txt
  147  help tac
  148  whatis tac
  149  tac --help
  150  grep "1" t3.txt
  151  zip t.zip t1.txt t2.txt t3.txt
  152  ls
  153  unzip t.zip
  154  sudo
  155  sudo --help
  156  sudo --help | more
  157  sudo -i
  158  head -1 t3.txt
  159  tail -1 t3.txt
  160  history
  161  history | more
  162  help
  163  help | more
  164  man vim
  165  dir
  166  ls
  167  ls -ltr
  168  cd
  169  cd .
  170  cd ~
  171  cd ..
  172  ls
  173  cd field
  174  ls
  175  cd $HOME
  176  pwd
  177  clear
  178  whereis
  179  whereis whereis
  180  whereis zip
  181  hep
  182  help
  183  whreeis help
  184  whereis cat
  185  whatis whatis
  186  whatis cat
  187  whatis help
  188  whatis zip
  189  shutdown --help
  190  shutdown
  191  shutdown -c
  192  shutdown -H
  193  history
  194  history | more
  195  dir
  196  ls
  197  dir
  198  cd Downloads
  199  dir
  200  ls
  201  sudo gedit test.txt
  202  vi test.txt
  203  clear
  204  alias
  205  alias --help
  206  clear
  207  dir
  208  cd ..
  209  dir
  210  cd opt
  211  dir
  212  rm -rf *.*
  213  dir
  214  https://javadl.oracle.com/webapps/download/AutoDL?BundleId=239835_230deb18db3e4014bb8e3e8324f81b43
  215  sudo wget https://javadl.oracle.com/webapps/download/AutoDL?BundleId=239835_230deb18db3e4014bb8e3e8324f81b43
  216  dir
  217  ls
  218  cd jdk1.8.0_251/
  219  dir
  220  cd ..
  221  ls
  222  dir
  223  cd ..
  224  sudo gedit .bash_profile 
  225  source .bash_profile
  226  java -version
  227  ls
  228  sudo gedit .bash_profile 
  229  source  .bash_profile
  230  java -version
  231  sudo gedit .bash_profile 
  232  source  .bash_profile
  233  java -version
  234  sudo gedit .bash_profile 
  235  source  .bash_profile
  236  sudo gedit .bash_profile 
  237  java -version
  238  cd opt/jdk1.8.0_251/
  239  ls
  240  ls bin
  241  ls jre
  242  ls lib
  243  ls man
  244  dir
  245  pwd
  246  cd
  247  cd opt/jdk1.8.0_251/
  248  pwd
  249  cd
  250  source .bash_profile 
  251  java -version 
  252  whereis jaca
  253  whereis java
  254  cd /usr/share/java
  255  dir
  256  rm *.*
  257  sudo rm *.*
  258  ls
  259  cd ..
  260  rmdir java
  261  sudo rmdir java
  262  ls
  263  cd java
  264  cd ~
  265  cd opt
  266  cd ..
  267  java -version
  268  path
  269  echo $PATH
  270  source .bash_profile
  271  java -version
  272  whereis java
  273  source .bash_profile
  274  java -version
  275  cd opt/
  276  ls 
  277  rm AutoDL\?BundleId\=239835_230deb18db3e4014bb8e3e8324f81b43 
  278  ls 
  279  cd jdk1.8.0_251/
  280  ls 
  281  cdd 
  282  cd
  283  gedit .bash_profile 
  284  sudo gedit .bash_profile 
  285  source .bash_profile 
  286  java -version
  287  reboot
  288  cd opt
  289  ls
  290  cd scala-2.11.8/
  291  pwd
  292  sudo apt-get update
  293  sudo apt-get install git-core
  294  git status
  295  git config --global user.name "sshia1"
  296  git config --global user.email "sshia1@outlook.com"
  297  git config list
  298  ssh-keygen -C "sshia1@outlook.com"
  299  ssh-add
  300  cd Documents
  301  ls
  302  ls -l
  303  git -clone "https://github.com/sshia1/bigdata_Plumbers"
  304  git clone "https://github.com/sshia1/bigdata_Plumbers"
  305  ls -a
  306  cd bigdata_Plumbers/
  307  ls -a
  308  cd ..
  309  cd abc/
  310  cd bigdata_Plumbers
  311  ls
  312  sudo gedit readme.txt
  313  ls
  314  cat readme.txt
  315  ls
  316  git add .
  317  git commit -m "readme file"
  318  git push origin master
  319  la -a
  320  mkdir 001_installation
  321  mkdir 002_wordCount
  322  git add .
  323  git commit -m "update"
  324  git push origin master
  325  la -a
  326  cd 002_wordCount
  327  jps
  328  git add .
  329  git commit -m "update"
  330  git push origin master
  331  cd ..
  332  cd 001_installation
  333  echo "Readme" > readme.txt
  334  cd ..
  335  cd..
  336  cd ..
  337  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  338  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  339  chmod 0600 ~/.ssh/authorized_keys
  340  wget http://mirror.cc.columbia.edu/pub/software/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz
  341  tar xzf hadoop-3.1.3.tar.gz 
  342  ls
  343  rm hadoop-3.1.3.tar.gz
  344  cd hadoop-3.1.3/
  345  pwd
  346  cd etc
  347  ls
  348  cd hadoop
  349  ls
  350  sudo gedit hadoop-env.sh
  351  sudo gedit core-site.xml 
  352  sudo gedit mapred-site.xml
  353  sudo gedit hdfs-site.xml 
  354  sudo gedit .bash_profile
  355  ls
  356  gedit .bash_profile
  357  source .bash_profile
  358  hdfs namenode -format
  359  source .bash_profile
  360  hdfs namenode -format
  361  ecoh $HADOOP_YARN_HOME
  362  echo $HADOOP_YARN_HOME
  363  set
  364  set | more
  365  hdfs namenode -format
  366  cd opt
  367  cd hadoop-3.1.3/
  368  cd etc
  369  cd hadoop
  370  grep "HADOOP_YARN_HOME"
  371  grep "HADOOP_YARN_HOME"*.*
  372  grep HADOOP_YARN_HOME .
  373  grep HADOOP_YARN_HOME *.*
  374  cd ..
  375  grep -r "HADOOP_YARN_HOME" *.*
  376  grep -help
  377  grep --help
  378  grep -r "HADOOP_YARN_HOME" *.*
  379  grep  "HADOOP_YARN_HOME" *.*
  380  ls
  381  cd etc
  382  ls
  383  cd hadoop
  384  grep "HADOOP_YARN_HOME" *.*
  385  echo $HADOOP_MAPRED_HOME
  386  set HADOOP_YARN_HOME=$HADOOP_MAPRED_HOME
  387  cd ..
  388  hdfs namenode -format
  389  echo $HADOOP_YARN_HOME
  390  cd opt
  391  ls
  392  cd hadoop-3.1.3/
  393  ls
  394  cd etc
  395  cd hadoop
  396  ls
  397  cd ..
  398  ls
  399  cd hdfs
  400  cd namenode
  401  pwd
  402  cd ..
  403  ls
  404  cd ..
  405  cd etc
  406  ls
  407  cd hadoop
  408  ls
  409  sudo edit yarn-site.xml
  410  sudo gedit yarn-site.xml
  411  cd ..
  412  ls
  413  source .bash_profile
  414  hdfs namenode -format
  415  ls
  416  cd opt
  417  cd hadoop-3.1.3/
  418  cd hdfs
  419  ls
  420  cd ..
  421  ls
  422  cd ..
  423  hdfs namenode -format
  424  source .bash_profile
  425  hdfs namenode -format
  426  path
  427  echo $PATH
  428  cat .bash_profile
  429  sudo gedit .bash_profile
  430  source .bash_profile
  431  path
  432  echo $PATH
  433  hdfs namenode -format
  434  cd opt
  435  cd hadoop-3.1.3/
  436  cd etc
  437  cd hadoop
  438  ls
  439  sudo gedit yarn-site.xml
  440  ls
  441  cat core-site.xml
  442  cd ..
  443  cd ~
  444  hdfs namenode -format
  445  ls
  446  cd opt/hadoop-3.1.3/
  447  cd etc/hadoop
  448  ls
  449  cat yarn-site.xml
  450  echo $PATH
  451  cd ..
  452  source .bash_profile
  453  hdfs namenode -format
  454  cd opt/hadoop-3.1.3/etc/hadoop
  455  ls
  456  cat yarn-en.sh
  457  cat yarn-env.sh
  458  ls
  459  mv hadoop-3.1.3/ opt
  460  cd opt
  461  ls
  462  cd hadoop-3.1.3/
  463  mkdir hdfs
  464  cd hdfs
  465  ls
  466  mkdir namenode
  467  mkdir datanode
  468  ls
  469  cd ..
  470  cd hadoop
  471  ls
  472  cd etc
  473  ls
  474  cd hadoop
  475  sudo gedit hdfs-site.xml
  476  sudo gedit download_hadoop.sh
  477  ls
  478  download_hadoop
  479  download_hadoop.sh
  480  source download_hadoop.sh
  481  ls
  482  cd opt
  483  ls
  484  rm -rf hadoop-3.1.3/
  485  ls
  486  mv ../hadoop-3.1.3.tar.gz .
  487  ls
  488  tar xzf hadoop-3.1.3.tar.gz 
  489  ls
  490  cd hadoop-3.1.3
  491  ls
  492  mkdir hdfs
  493  cd hdfs
  494  mkdir namenode
  495  mkdir datanode
  496  cd ..
  497  ls
  498  cd etc/hadoop
  499  ls
  500  sudo gedit hadoop-env.sh
  501  cd ~
  502  cat .bash_profile
  503  cd opt
  504  cd hadoop-3.1.3/
  505  cd etc/hadoop
  506  sudo gedit hadoop-env.sh
  507  cd ..
  508  pwd
  509  cd etc/hadoop
  510  sudo gedit mapred-site.xml
  511  cd etc/hadoop
  512  ls
  513  sudo gedit yarn-site.xml
  514  cat hdfs-site.xml
  515  ls
  516  cat hdfs-site.xml 
  517  sudo gedit hdfs-site.xml 
  518  cat hadoop_env.sh
  519  cat hadoop-env.sh
  520  cd ~
  521  ls
  522  cat .bash_profile
  523  source .bash_profile
  524  hdfs namenode -format
  525  start-all.sh
  526  jps
  527  cd opt
  528  cd hadoop-3.1.3/
  529  ls
  530  cd hdfs
  531  ls
  532  ls datanode
  533  ls namenode
  534  cd ..
  535  cd etc/hadoop
  536  ls
  537  cat core-site.xml
  538  history | more
  539  cd ~
  540  cd Documents
  541  ls
  542  cd bigdata_Plumbers/
  543  ls
  544  history | more
  545  cd opt
  546  ls
  547  cd Python-3.8.3
  548  source .bash_profile
  549  path
  550  echo $PYTHON_HOME
  551  cd ..
  552  source .bash_profile
  553  cat .bash_profile
  554  echo $PYTHON_HOME
  555  ls
  556  cd Downloads
  557  ls
  558  pwd
  559  mv 'hr - hr.csv' hr-hr.csv
  560  ls
  561  cd/
  562  cd .
  563  cd ..
  564  cd /etc/my.cnf
  565  /usr/bin
  566  cd /usr/bin/mysql
  567  ls
  568  cd etc
  569  cd mysql
  570  ls
  571  sudo gedit mysql.cnf
  572  sudo gedit my.cnf
  573  pwd
  574  ls
  575  mv hr-hr.csv hr-hr.csv.bak
  576  mv 'hr - hr.csv' hr-hr.csv
  577  cat hr-hr.csv | more
  578  ls *.csv
  579  ls
  580  ls -l
  581  ls
  582  cd ..
  583  ls
  584  cd Documents
  585  ls
  586  cd ..
  587  ls
  588  cd Downloads
  589  ls
  590  cat Employees.csv | more
  591  ls -l
  592  ls
  593  mv 'db_plumbers - employee.csv' db_plumbers-employee.csv
  594  mv 'db_plumbers - hr.csv' db_plumbers-hr.csv
  595  ls
  596  cd ..
  597  ls
  598  la -a
  599  cd Downloads
  600  ls
  601  exit
  602  cd opt/hadoop-3.1.3/
  603  pwd
  604  cd ..
  605  ls
  606  cd jdk1.8.0_221
  607  pwd
  608  cd ~
  609  java -version
  610  cd opt
  611  cd hadoop-3.1.3/
  612  cd etc/hadoop
  613  ls
  614  sudo gedit core-site.xml
  615  sudo gedit hdfs-site.xml
  616  ls
  617  cd ..
  618  ls
  619  cd hdfs
  620  ls
  621  cd $HOME
  622  sudo apt update
  623  sudo apt install software-properties-common
  624  sudo add-apt-repository ppa:deadsnakes/ppa
  625  sudo apt install python3.7
  626  python3.7 --version
  627  cat .bash_profile
  628  la -a
  629  cd Downloads
  630  ls
  631  cd ..
  632  cd opt
  633  ls
  634  cd ..
  635  ls
  636  path
  637  echo $PATH
  638  python3.7
  639  whereis python3.8
  640  whereis python3.7
  641  jps
  642  source .bash_profile
  643  start-all.sh
  644  jps
  645  hdfs dfs -mkdir /temp
  646  hdfs dfs -ls /
  647  cd Documents/
  648  ls
  649  echo "test" > test.txt
  650  ls
  651  mv test.txt test1.txt
  652  hdfs dfs -put test1.txt /temp
  653  cd opt/hadoop-3.1.3/
  654  cd lib
  655  ls
  656  cd ../opt/hadoop-3.1.3
  657  cd lib
  658  ls
  659  cd $HOME/Documents
  660  hdfs dfs -put test1.txt /temp1
  661  hdfs dfs -ls /temp
  662  hdfs dfs -cat /temp/test1.txt
  663  cd $HOME/Downloads
  664  ls *.txt
  665  ls
  666  cd ..
  667  ls
  668  cd Documents
  669  ls
  670  cd ..
  671  cd Downloads
  672  ls
  673  cd ..
  674  ls
  675  cd Documents
  676  ls
  677  cd ..
  678  cd Downloads
  679  ls
  680  hdfs dfs -ls
  681  hdfs dfs -ls /
  682  hdfs dfs -ls /temp
  683  hdfs dfs -ls /temp1
  684  hdfs dfs -ls /
  685  hdfs dfs -mkdir /temp2
  686  hdfs dfs -ls /
  687  hdfs dfs -mv Shakespeare.txt /temp2
  688  ls *.txt
  689  hdfs dfs -put Shakespeare.txt /temp2
  690  hdfs dfs -ls /temp2
  691  hdfs dfs -ls /
  692  source .bash_profile
  693  start-all.sh
  694  start
  695  cd ..
  696  start
  697  start-yarn.sh
  698  start-dfs.sh
  699  jps
  700  start-yarn.cmd
  701  whereis start-yarn.cmd
  702  pip install --upgrade pip
  703  pip3 install --upgrade pip
  704  sudo apt install python3-pip
  705  pip3 install --upgrade pip
  706  pip3 install --upgrade -r requirements.txt
  707  cat .bash_profile
  708  python
  709  python3
  710  whereis python3
  711  cd /usr/bin/python3
  712  cd /usr/bin
  713  ls
  714  ls python*.*
  715  cd/$HOME
  716  cd /$HOME
  717  cat .bash_profile
  718  pip3 install pydoop
  719  sudo gedit wc.py
  720  sudo gedit stop.txt
  721  pydoop script wc.py hdfs_input hdfs_output --upload-file-to-cache stop.txt
  722  sudo apt-get install build-essential python-dev
  723  echo $HADOOP_HOME
  724  python3 setup.py build
  725  pydoop script wc.py hdfs_input hdfs_output --upload-file-to-cache stop.txt
  726  pip3 install pydoop
  727  cd /usr/lib/python3
  728  ls
  729  cd dist-packages
  730  ls
  731  ls py*.*
  732  ls
  733  ls | more
  734  ls
  735  pip3 install pydoop
  736  whereis python3
  737  cat .bash_profile
  738  sudo apt-get install build-essential checkinstall
  739  sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev     libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev libffi-dev zlib1g-dev
  740  cd opt
  741  sudo wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz
  742  sudo tar xzf Python-3.8.3.tgz 
  743  ls
  744  cd Python-3.8.3/
  745  sudo ./Configure --enable-optimizations
  746  sudo ./configure --enable-optimizations
  747  sudo make altinstall
  748  python 3.8 -V
  749  python3.8 -V
  750  cd ..
  751  ls
  752  rm Python-3.8.3.tgz
  753  ls
  754  cd ..
  755  cat .bash_profile
  756  sudo gedit .bash_profile
  757  cat .bash_profile
  758  echo $PYTHON_HOME
  759  source .bash_profile
  760  echo $PATH
  761  python3.8 -V
  762  pip install mrjob
  763  pip3 install mrjob
  764  sudo gedit mr_word_count.y
  765  ls
  766  mv mr_word_count.y mr_word_count.py
  767  ls
  768  python3.8.3 mr_word_count.py stop.txt
  769  ls
  770  cd opt
  771  ls
  772  python3.8.3
  773  cd Python-3.8.3
  774  cd bin
  775  ls
  776  cd ..
  777  ls
  778  python3.8 mr_word_count.py stop.txt
  779  ls
  780  cd Documents
  781  ls
  782  cd ..
  783  cd Downloads
  784  ls
  785  cp Shakespeare.txt ..
  786  cd ..
  787  ls
  788  sudo gedit Shakespeare.txt
  789  sudo gedit test.txt
  790  cat test.txt
  791  ls
  792  sudo gedit test.py
  793  python3.8 test.py test.txt
  794  cat test.txt
  795  hdfs dfs -put test.txt /temp2
  796  hdfs dfs -ls /temp2
  797  hdfs dfs -pwd /temp2
  798  hdfs dfs -ls /temp2
  799  python3.8 test.py test.txt
  800  python3.8 test.py test.txt
  801  hdfs dfs -ls /temp2
  802  hdfs dfs -ls 
  803  pwd
  804  hdfs dfs -ls /user/field/temp2
  805  hdfs dfs -ls /
  806  hadoop fs -ls /
  807  hadoop fs -ls /field
  808  hadoop fs -ls hdfs://
  809  hadoop fs -ls hdfs:///temp2
  810  hadoop fs -ls hdfs:///temp2/teset.txt
  811  hadoop fs -ls hdfs:///temp2/test.txt
  812  python3.8 test.py -r hadoop hdfs:///temp2/test.txt
  813  hadoop fs -ls hdfs:///temp2/test.txt
  814  python3.8 test.py test.txt --output-dir hdfs://temp2/test.out
  815  python3.8 test.py test.txt --output-dir hdfs:///temp2/test.out
  816  hdfs dfs -ls hdfs:///temp2
  817  ls
  818  cat test.py
  819  python3.8 test.py test.txt 
  820  cp test.py mapreduce.py
  821  ls *.txt
  822  python3.8 mapreduce.py Shakespeare.txt
  823  python3.8 mapreduce.py Shakespeare.txt > t.out
  824  cat t.out | more
  825  sudo apt-get update
  826  sudo apt-get install mysql server
  827  sudo apt-get install mysql -server5.7
  828  sudo apt-get install mysql -server
  829  cd Downloads
  830  ls *.odt
  831  cat 'mySQL server Install.odt' | more
  832  cd ..
  833  cd Downloads
  834  dir *.odt
  835  clear
  836  cls
  837  ls
  838  sudo apt-get mysql-server
  839  sudo apt-get install mysql-server
  840  sudo ufw enable
  841  sudo ufw allow mysql
  842  sudo systemctl start mysql
  843  systemctl status mysql service
  844  update mysql.user set authentication_string=password('password') where user='root'
  845  update mysql.user set authentication_string=password('password') where user='root';
  846  UPDATE mysql.user SET authentication_string=PASSWORD('password') where user='root';
  847  UPDATE mysql.user SET authentication_string = PASSWORD('password') where user='root';
  848  UPDATE mysql.user SET authentication_string = PASSWORD('password') where User='root';
  849  UPDATE mysql.user SET authentication_string = PASSWORD('password') where User = 'root';
  850  UPDATE mysql.user SET authentication_string = PASSWORD("'password'") where User = 'root';
  851  UPDATE mysql.user SET authentication_string = "PASSWORD"('password') where User = 'root';
  852  UPDATE mysql.user SET authentication_string = "PASSWORD(password)" where User = 'root';
  853  mysql
  854  /usr/bin/mysql -u root -p
  855  sudo /usr/bin/mysql -u root -p
  856  ls
  857  exit
  858  ls
  859  mv "100 Records.csv" hr.csv
  860  ls
  861  sudo /usr/bin/mysql -u root -p
  862  sudo mysql --local-infile=1 -u root -pl
  863  exit
  864  ls
  865  wget -qO- https://deb.opera.com/archive.key | sudo apt-key add -
  866  sudo add-apt-repository "deb [arch=i386,amd64] https://deb.opera.com/opera-stable/ stable non-free"
  867  sudo apt install opera-stable
  868  ls
  869  sudo /usr/bin/mysql -u root -p
  870  mysql --local-infile=1 -u root -p1
  871  sudo mysql --local-infile=1 -u root -p1
  872  ls
  873  cd Documents
  874  ls
  875  dir
  876  ls -l
  877  cd ..
  878  cd Documents
  879  ls
  880  cd bigdata_Plumbers
  881  ls
  882  mkdir 003_MRJOBS_MapReduce
  883  mkdir 004_MySQL
  884  mkdir 000_IBM_Certifications
  885  cd ~/Downloads
  886  ls
  887  cp db_plumbers-employee.csv ../Documents/bigdata_Plumbers
  888  cp db_plumbers-hr.csv ../Documents/bigdata_Plumbers
  889  cp SQL-solution-John-Shia.odt ../Documents/bigdata_Plumbers
  890  ls
  891  cd ..
  892  ls
  893  cp mapreduce.py Documents/bigdata_Plumbers
  894  cp Shakespeare.txt Documents/bigdata_Plumbers
  895  more < t.out
  896  ls t.out
  897  mv t.out mapreduce_output.txt
  898  mv mapreduce_output.txt Documents/bigdata_Plumber
  899  ls
  900  cd Documents
  901  ls
  902  cd ..
  903  ls
  904  cd Downloads
  905  ls
  906  cd ..
  907  cd Documents
  908  ls
  909  cat bigdata_Plumber
  910  ls
  911  mv bigdata_Plumber mapreduce_output.txt
  912  ls
  913  cat test1.txt
  914  mv 'IBMDeveloperSkillsNetwork CC0101EN Certificate _ Cognitive Class.pdf' bigdata_Plumbers
  915  mv 'Cognitive Class BD0101EN Certificate _ Cognitive Class.pdf' bigdata_Plumbers
  916  ls
  917  mv mapreduce_output.txt bigdata_Plumbers
  918  cd bigdata_Plumbers
  919  ls
  920  mv 'Cognitive Class BD0101EN Certificate _ Cognitive Class.pdf'  000_IBM_Certifications/
  921  mv 'IBMDeveloperSkillsNetwork CC0101EN Certificate _ Cognitive Class.pdf' 000_IBM_Certifications/
  922  ls
  923  mv db_plumbers-employee.csv 004_MySQL
  924  mv db_plumbers-hr.csv 004_MySQL
  925  mv mapreduce.py 003_MRJOBS_MapReduce/
  926  mv mapreduce_output.txt 003_MRJOBS_MapReduce/
  927  ls
  928  mv Shakespeare.txt 003_MRJOBS_MapReduce/
  929  mv SQL-solution-John-Shia.odt 004_MySQL
  930  ls
  931  git add .
  932  git commit -m "update"
  933  git push origin master
  934  history > history.out
  935  sudo gedit history.out
  936  ls
  937  git add 000_IBM_Certifications
  938  git add 003_MRJOBS_MapReduce/
  939  git add 002_wordCount/
  940  git commit -m "update"
  941  git push
  942  git pull
  943  cd 001_installation/readme.txt
  944  cd 001_installation/
  945  ls
  946  cat readme.txt
  947  del readme.txt
  948  rm -rf readme.txt
  949  ls
  950  cd ..
  951  git pull
  952  git add/rm
  953  git --help
  954  git merge
  955  git add 001_installation/
  956  git pull
  957  ls
  958  la -a
  959  cd 001_installation
  960  ls
  961  la -a
  962  ls
  963  cd ..
  964  git add .
  965  git commit -m "update"
  966  git push origin master
  967  cd ..
  968  sudo apt-get update
  969  nc 
  970  nc localhost 9600 -l
  971  ls *.sh
  972  cat hadoopstatus.sh
  973  ls *.sh
  974  cp download_hadoop.sh Documents/bigdata_Plumbers/
  975  cd Documents/bigdata_Plumbers/
  976  ls
  977  copy download_hadoop.sh 100_SCRIPTS
  978  cp download_hadoop.sh 100_SCRIPTS/
  979  ls
  980  exit
  981  sudo gedit listports.sh
  982  hostname -I
  983  . listport.sh 192.168.0.175 0 1023
  984  ls
  985  . listports.sh 192.168.0.175 0 1023
  986  ./listports.sh 192.168.0.175 1023
  987  chmod +x listports.sh
  988  sudo chmod +x listports.sh
  989  ./listports.sh 192.168.0.175 1023
  990  sudo gedit listports.sh
  991  cd Documents/bigdata_Plumbers
  992  cd 100_SCRIPTS/
  993  ls
  994  cat hadoopstatus.sh
  995  . listports.sh
  996  cat listports.sh
  997  cd ~
  998  ls
  999  la -s
 1000  ls
 1001  cd Documents/bigdata_Plumbers
 1002  ls
 1003  cd ~
 1004  ls
 1005  ls listports.sh
 1006  . listports.sh
 1007  . listports.sh | more
 1008  sudo gedit listports.sh
 1009  . listports.sh
 1010  . listports.sh > t.out
 1011  sudo gedit t.out
 1012  cat listports.sh
 1013  hostname -I
 1014  start-all.sh
 1015  jps
 1016  ls
 1017  . hadoopstatus.sh
 1018  sudo gedit hadoopstatus.sh
 1019  . hadoopstatus.sh
 1020  sudo gedit hadoopstatus.sh
 1021  . hadoopstatus.sh
 1022  sudo gedit hadoopstatus.sh
 1023  . hadoopstatus.sh
 1024  cd opt
 1025  cd hadoop
 1026  ls
 1027  cd hadoop-3.1.3
 1028  ls
 1029  cd etc
 1030  ls
 1031  cd hadoop
 1032  ls
 1033  cd ..
 1034  cd bin
 1035  ls
 1036  cd hadoop
 1037  cd ..
 1038  ls
 1039  cd sbin
 1040  ls
 1041  cd ~
 1042  . stop-all.sh
 1043  . hadoopstatus.sh
 1044  copy hadoopstatus Documents/bigdata_Plumbers
 1045  copy hadoopstatus Documents/bigdata_Plumber
 1046  cp hadoopstatus.sh Documents/bigdata_Plumber
 1047  cd Documents/bigdata_Plumber
 1048  cd Documents
 1049  ls
 1050  mv bigdata_Plumber hadoopstatus.sh
 1051  cat hadoopstatus.sh
 1052  mv hadoopstatus.sh bigdata_Plumbers
 1053  ls
 1054  cd bigdata_Plumbers
 1055  ls
 1056  cp hadoopstatus.sh 100_SCRIPTS/
 1057  cp hadoopstatus.sh 005_BASH_CLUSTER_Setup/
 1058  ls
 1059  cp updategit.sh 100_SCRIPTS/
 1060  . updategit.sh
 1061  ls
 1062  sudo gedit getgit.sh
 1063  ls
 1064  cp getgit.sh 100_SCRIPTS/
 1065  cp *.sh 100_SCRIPTS/
 1066  rm download_hadoop.sh 
 1067  rm hadoopstatus.sh
 1068  ls
 1069  cat updategit.sh
 1070  . updategit.sh
 1071  ls
 1072  ls 110_CERTIFICATES/
 1073  ls 
 1074  . getgit.sh
 1075  ls
 1076  ls 110_CERTIFICATES/
 1077  cd ..
 1078  history > history.out
 1079  sudo gedit history.out
 1080  nc localhost 9600
 1081  nc localhost 9600 -l
 1082  sudo apt-update
 1083  nc localhost 9600 -l
 1084  ip addr
 1085  cd Documents
 1086  ls
 1087  cd bigdata_Plumbers
 1088  ls
 1089  mkdir 005_BASH_CLUSTER_Setup
 1090  cd 005_BASH_CLUSTER_Setup/
 1091  echo "">readme.txt
 1092  cd ..
 1093  git add .
 1094  git commit -m "update"
 1095  git push origin master
 1096  ip addr show
 1097  ipconfig
 1098  ifconfig
 1099  sudo apt instlal net-tools
 1100  sudo apt install net-tools
 1101  ifconfig
 1102  hostname -I
 1103  ip addr show
 1104  ifconfig
 1105  hostname -I
 1106  service iptables stop
 1107  sudo chkconfig
 1108  sudo ufw disable
 1109  ls
 1110  cd ..
 1111  cd bigdata_Plumbers
 1112  ls
 1113  mkdir 100_SCRIPTS
 1114  mkdir 101_NOTES
 1115  mkdir 102_MATERIAL
 1116  echo "" > 100_SCRIPTS/readme.txt
 1117  echo "" > 101_NOTES/readme.txt
 1118  echo ""> 102_MATERIAL/readme.txt
 1119  dir
 1120  sudo gedit update_git.sh
 1121  ls
 1122  . update_git.sh
 1123  ls
 1124  mkdir 110 CERTIFICATES_GENERAL
 1125  ls
 1126  rmdir 110
 1127  rmdir CERTIFICATES_GENERAL/
 1128  mkdir 102_CERTIFICATES_GENERAL
 1129  ls
 1130  rmdir 102_CERTIFICATES_GENERAL/
 1131  ls
 1132  mkdir 110_CERTIFICATES
 1133  echo ""> 110_CERTIFICATES/readme.txt
 1134  ls 004_MySQL
 1135  update_git
 1136  . update_git.sh
 1137  mv update_git.sh updategit.sh
 1138  sudo gedit updategit.sh
 1139  updategit.sh
 1140  . updategit.sh
 1141  ls
 1142  cat updategit.sh
 1143  . updategit.sh
 1144  cd ..
 1145  ls
 1146  cat download_hadoop.sh
 1147  cd Downloads
 1148  ls
 1149  ls *.sh
 1150  cd ..
 1151  ls *.sh
 1152  history > t.out
 1153  sudo gedit t.out
 1154  sudo gedit hadoopstatus.sh
 1155  jps
 1156  ls
 1157  sudo gedit hadoopstatus.sh
 1158  . hadoopstatus.sh
 1159  sudo gedit .bash_profile
 1160  source .bash_profile
 1161  sqoop version
 1162  mysql -u user -p -e 'SHOW TABLES FROM data_plumber;'
 1163  sudo mysql -u user -p -e 'SHOW TABLES FROM data_plumber;'
 1164  java --version
 1165  java -version
 1166  hadoop version
 1167  cd Documents/bigdata_Plumber
 1168  ls
 1169  cd Documents
 1170  ls
 1171  cd bigdata_Plumbers
 1172  ls
 1173  mkdir 006_Sqoop_and_CodingPractice
 1174  echo "" > 006_Sqoop_and_CodingPractice/readme.txt
 1175  dir *.sh
 1176  . updategit.sh
 1177  hostname -I
 1178  hostname -i
 1179  hostname -I
 1180  hostname -help
 1181  cls
 1182  clear
 1183  hostname -I
 1184  nc 192.168.0.175 1234
 1185  nc 10.0.2.15 1234
 1186  nc -l 1234
 1187  nc 192.168.0.175 1234 -l
 1188  hostname -I
 1189  nc -zv 192.168.0.175
 1190  nc -zv host.example.com 20-30
 1191  sudo apt-get install netcat
 1192  nc -l -p 31337
 1193  ifconfig
 1194  nc -l -p 31337
 1195  netcat -z -n -v 192.168.0.192 1-1000 2>&1 | grep succeeded
 1196  netcat -z -n -v 192.168.0.192 1-1000
 1197  netcat -z -n -v 192.168.0.192 1-1000 > t.out
 1198  netcat -z -n -v 192.168.0.192 1-1000 >& t.out
 1199  more < t.out
 1200  nc 192.168.0.192 2222
 1201  sudo lsof -i -P -n | grep LISTEN
 1202  sudo lsof -i -P -n | more
 1203  cls
 1204  clear
 1205  fdasfasd
 1206  ifconfig
 1207  nc 192.168.0.192 9600 -l
 1208  nc 127.0.0.1 9600 -l
 1209  sudo services ssh status
 1210  ssh rahib@127.0.0.1
 1211  nc 127.0.0.1 9600 -l
 1212  nc 192.168.0.192 9600 -l
 1213  ifconfig
 1214  nc -l 9600
 1215  hostname -I
 1216  nc -v -w 192.168.0.175 9600
 1217  sudo apt flameshot
 1218  sudo apt-install flameshot
 1219  sudo apt install flameshot
 1220  nc 192.168.0.192 9600
 1221  hello
 1222  nc 192.168.0.192 9600
 1223  cat download_hadoop.sh
 1224  nc 10.0.2.15 9600
 1225  hello
 1226  how are you?
 1227  ipconfig
 1228  ifconfig
 1229  ip address
 1230  nc 9600 -l
 1231  nc 10.0.2.15 9600
 1232  nc  9600
 1233  nc 9600 -l
 1234  nc 10.1.10.205 9600 -l
 1235  ip address
 1236  ifconfig
 1237  ifconfig | grep "inet "
 1238  ip address | grep "inet "
 1239  hostname -I | grep "inet "
 1240  hostname -I
 1241  nc 9600 -l
 1242  nc 10.0.2.15 9600 -l
 1243  nc 10.0.2.15 9600
 1244  ip address
 1245  nc 10.1.10.205 9600 -l
 1246  ip address
 1247  nc 10.0.2.15 9600
 1248  nc 10.1.10.205 9600
 1249  ip address
 1250  nc 10.1.10.205 9600 -l
 1251  nc 10.1.10.222 9600
 1252  sudo vi test.txt
 1253  history>t.out
 1254  sudo gedit t.out
 1255  cd .ssh
 1256  cat authorized_keys 
 1257  cd ..
 1258  cd /etc
 1259  cat hosts
 1260  cat hostname
 1261  cd $HOME
 1262  cd opt
 1263  ls
 1264  cd hadoop-3.1.3
 1265  cd etc/hadoop
 1266  ls workers
 1267  cat workers
 1268  cd/
 1269  cd ~
 1270  sudo apt-update
 1271  sudo apt-install update
 1272  history | grep "sudo"
 1273  history | grep "sudo apt"
 1274  sudo apt update
 1275  cd opt
 1276  wget http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
 1277  ls
 1278  sudo tar xvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 
 1279  ls
 1280  rm -rf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
 1281  ls
 1282  mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop-1.4.7
 1283  ls
 1284  cd sqoop-1.4.7/
 1285  pwd
 1286  sudo gedit .bashrc
 1287  cd ..
 1288  sudo gedit .bashrc
 1289  cd opt
 1290  cd sqoop-1.4.7/
 1291  ls
 1292  cd conf
 1293  ls
 1294  mv sqoop-env-template.sh sqoop-env.sh
 1295  ls
 1296  sudo gedit sqoop-env.sh
 1297  sqoop
 1298  echo $PATH
 1299  sqoop version
 1300  cd $HOME
 1301  source .bash_profile
 1302  sqoop version
 1303  sudo gedit .bashrc
 1304  cd /var/lib/mysql
 1305  sudo cd /var/lib/mysql
 1306  sudo ls /var/lib/mysql
 1307  history | grep "mysql"
 1308  sudo mysql --local-infile=1 -u root -p1
 1309  cd opt
 1310  ls
 1311  sudo tar -xvf eclipse-inst-linux64.tar.gz 
 1312  ls
 1313  sudo snap install eclipse --classic
 1314  ls
 1315  sudo tar -xvzf ~/Downloads/eclipse-jee-2019-09-R-linux-gtk-x86_64.tar.gz
 1316  cd eclipse-installer
 1317  ls
 1318  cd ..
 1319  rm -rf eclipse-installer
 1320  sudo rm -rf eclipse-inst
 1321  ls
 1322  sudo rm -rf eclipse-installer
 1323  ls
 1324  sudo tar -xvzf ~/Downloads/eclipse-jee-2019-09-R-linux-gtk-x86_64.tar.gz
 1325  ls
 1326  sudo tar -xvzf eclipse-inst-linux64.tar.gz 
 1327  ls eclipse.desktop
 1328  ls
 1329  cd eclipse-installer
 1330  ls
 1331  eclipse-inst
 1332  cat eclipse-inst
 1333  ls .bashrc
 1334  sudo gedit .bashrc
 1335  exit
 1336  cd opt
 1337  ls
 1338  cd flume-1.8.0/
 1339  cd conf
 1340  ls
 1341  cp flume-conf1.properties  flume-conf2.properties
 1342  sudo cp flume-conf1.properties  flume-conf2.properties 
 1343  ls
 1344  sudo gedit flume-conf2.properties 
 1345  sudo cp flume-conf1.properties  flume-conf2.properties 
 1346  sudo gedit flume-conf2.properties
 1347  ls
 1348  ls flutest
 1349  pwd
 1350  ls
 1351  sudo gedit flumetest
 1352  pwd
 1353  mkdir flutest
 1354  cd flutest
 1355  pwd
 1356  cd ..
 1357  jps
 1358  hdfs dfs -ls /
 1359  hdfs dfs -mkdir /flume01
 1360  hdfs dfs -ls /
 1361  clear
 1362  flume-ng agent -n plumber -C conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties.template 
 1363  cd opt
 1364  ls
 1365  cd flume-1.8.0
 1366  ls
 1367  cd conf
 1368  ls
 1369  sudo gedit flume-conf1.properties.template 
 1370  ls
 1371  sudo gedit flume-conf1.properties.template 
 1372  cd $HOME
 1373  flume-ng agent -n plumber -C conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties.template 
 1374  cd opt
 1375  ls
 1376  cd flume-1.8.0/
 1377  ls
 1378  cd conf
 1379  ls
 1380  mv flume-conf1.properties.template  flume-conf1.properties
 1381  sudo mv flume-conf1.properties.template  flume-conf1.properties
 1382  ls
 1383  sudo gedit flume-conf1.properties
 1384  cd $HOME
 1385  cd flutest
 1386  sudo gedit flu1
 1387  ls
 1388  cat flu1
 1389  cd ..opt
 1390  cd ../opt
 1391  ls
 1392  cd flume-1.8.0/
 1393  cd conf
 1394  ls
 1395  sudo gedit flume-conf1.properties 
 1396  cd $HOME
 1397  flume-ng -hellp
 1398  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties
 1399  flume-ng -help
 1400  cd opt/flume-1.8.0/conf
 1401  ls
 1402  sudo gedit flume-conf1.properties
 1403  cd $HOME
 1404  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties
 1405  jps
 1406  hdfs dfs -ls /
 1407  hdfs -dfs -ls /flume01
 1408  hdfs -dfs -ls /
 1409  hdfs -dfs ls /flume01
 1410  hdfs -dfs ls /
 1411  hdfs -dfs -ls /
 1412  hdfs dfs -ls /
 1413  hdfs dfs -ls /flume01
 1414  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties
 1415  ls
 1416  cd flumetest
 1417  cat flumetest
 1418  cat flutest
 1419  cd flutest
 1420  pwd
 1421  cd $HOME
 1422  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf2.properties
 1423  ls
 1424  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf2.properties
 1425  grep guava*.jar -r *.*
 1426  grep guava*.jar -r /*.*
 1427  grep guava*.jar -r /
 1428  whereis java
 1429  cd opt
 1430  cd jdk1.8.0_221/
 1431  grep -r guava*.*
 1432  cd ..
 1433  ls
 1434  grep guava*.*
 1435  grep -r guava*.jar
 1436  grep -help
 1437  grep --help
 1438  grep --help | more
 1439  ls
 1440  cd flume-1.8.0
 1441  ls
 1442  cd lib
 1443  ls
 1444  mkdir BAK
 1445  sudo mkdir BAK
 1446  ls
 1447  mv guava-11.0.2.jar  BAK
 1448  sudo mv guava-11.0.2.jar BAK
 1449  ls BAK
 1450  cd ..
 1451  cd opt
 1452  cd flume-1.8.0/
 1453  cd conf
 1454  ls
 1455  sudo gedit flume-conf1.properties 
 1456  sudo apt update
 1457  sudo apt install -y wget
 1458  sudo apt install -y openjdk-8-jdk
 1459  java -version
 1460  wget http://ftp.jaist.ac.jp/pub/eclipse/technology/epp/downloads/release/2019-03/R/eclipse-java-2019-03-R-linux-gtk-x86_64.tar.gz
 1461  sudo tar -zxvf eclipse-java-2019-*-R-linux-gtk-x86_64.tar.gz -C /usr/
 1462  sudo ln -s /usr/eclipse/eclipse /usr/bin/eclipse
 1463  sudo nano /usr/share/applications/eclipse.desktop
 1464  eclipse
 1465  sudo bigdata_Plumbers.sh
 1466  sudo gedit class.sh
 1467  ls
 1468  . class.sh
 1469  ls
 1470  cd 002_wordCount
 1471  ls
 1472  l mapreduce-wordcount-shia.py 
 1473  more < mapreduce-wordcount-shia.py 
 1474  ls
 1475  less python-mapreduce-data-flow.jpg 
 1476  man less
 1477  man list
 1478  sqoop version
 1479  sqoop -version
 1480  cd $HOME
 1481  sqoop import --connect jdbc:mysql://localhost/data_plumber --username root --table employee --m 1
 1482  sudo apt-get install mysql-connector-java*
 1483  sudo add-apt- repository ppa:webupd8team/java
 1484  sudo add-apt-repository ppa:webupd8team/java
 1485  sudo apt-get update
 1486  sudo apt-get install oracle-java8-installer
 1487  sudo rm /etc/apt/sources.list.d/webupd8team-java.list
 1488  sudo apt update
 1489  sudo add-apt-repository ppa:webupd8team/java
 1490  sudo apt-get update
 1491  sudo apt-get install oracle-java8-installer
 1492  sudo apt update && sudo apt upgrade
 1493  sudo add-apt-repository ppa:linuxuprising/java
 1494  sudo apt install oracle-java12-installer
 1495  java -version
 1496  cd Downloads
 1497  ls
 1498  mv mysql-connector-java-2.0.14.tar.gz ../opt
 1499  cd ../opt
 1500  ls
 1501  tar -zxf mysql-connector-java-2.0.14.tar.gz 
 1502  sudo tar -zxf mysql-connector-java-2.0.14.tar.gz 
 1503  cd ..
 1504  cd Downloads
 1505  ls
 1506  cd ..
 1507  sqoop import --connect jdbc:mysql://localhost/data_plumber --username root --table employee --m 1
 1508  cd opt
 1509  cd sqoop/lib
 1510  ls
 1511  cd sqoop-1.4.7/
 1512  ls
 1513  cd lib
 1514  ls
 1515  cd /bin/sqoop
 1516  whereis mysql-connector
 1517  sudo tar -zxf mysql-connector-java-2.0.14.tar.gz cd ~/opt
 1518  cd ~/opt
 1519  ls
 1520  cd mysql-connector-java-2.0.14/
 1521  ls
 1522  cp mysql-connector-java-2.0.14-bin.jar ~/opt/sqoop-1.4.7/lib
 1523  cd ~
 1524  sqoop import --connect jdbc:mysql://localhost/data_plumber --username root --table employee --m 1
 1525  history > t.out
 1526  su gedit t.out
 1527  sudo gedit t.out
 1528  history > hist000.out
 1529  ls
 1530  cd opt
 1531  ls
 1532  cd sqoop-1.4.7
 1533  ls
 1534  cd lib
 1535  ls
 1536  del mysql-connector-java-2.0.14-bin.jar 
 1537  rm mysql-connector-java-2.0.14-bin.jar 
 1538  cd ~
 1539  sqoop import --connect jdbc:mysql://localhost/data_plumber --root -m 1 --table employee 
 1540  sqoop import --connect jdbc:mysql://localhost/data_plumber --username root --table employee --m 1
 1541  java -version
 1542  java version
 1543  java -version
 1544  hadoop version
 1545  java -version
 1546  ls
 1547  . hadoopstatus.sh
 1548  . start-all.sh
 1549  . hadoopstatus.sh
 1550  cd Downloads
 1551  ls
 1552  v mysql-connector-java-8.0.20.jar ../opt
 1553  mv mysql-connector-java-8.0.20.jar ../opt
 1554  cd ../opt
 1555  ls
 1556  mv mysql-connector-java-2.0.14 sqoop-1.4.7/lib
 1557  mv mysql-connector-java-8.0.20.jar sqoop-1.4.7/lib/
 1558  cd sqoop-1.4.7/lib
 1559  ls
 1560  cd ~
 1561  sqoop import --connect jdbc:mysql://localhost/data_plumber --username root --table employee --m 1
 1562  sudo sqoop import --connect jdbc:mysql://localhost/data_plumber --username root --table employee --m 1
 1563  cd opt
 1564  sudo wget https://archive.apache.org/dist/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz
 1565  history | tar
 1566  histroy | grep "tar"
 1567  history | grep "tar"
 1568  sudo tar -zxvf apache-flume-1.8.0-bin.tar.gz 
 1569  ls
 1570  mv apache-flume-1.8.0-bin flume-1.8.0
 1571  ls
 1572  cd flume-1.8.0
 1573  pwd
 1574  cd $HOME
 1575  sudo gedit .bash_profile
 1576  source .bash_profile
 1577  path
 1578  echo $PATH
 1579  cd opt
 1580  ls
 1581  rm -f *.tar.gz
 1582  ls
 1583  rm scala-2.11.8.tgz
 1584  ls
 1585  cd ..
 1586  cd sbt-1.3.13.tgz
 1587  ls sbt-1.3.13.tgz
 1588  ls
 1589  source .bash_profile
 1590  flume-ng version
 1591  cd opt
 1592  ls
 1593  sudo rm -rf sbt-1.3.13.tgz
 1594  ls
 1595  cd ..
 1596  ls
 1597  cd opt/flume-1.8.0/conf
 1598  ls
 1599  cp flume-conf.properties.template flume-conf1.properties.template 
 1600  ls
 1601  sudo cp flume-conf.properties.template flume-conf1.properties.template 
 1602  ls
 1603  sudo gedit flume-conf1.properties.template 
 1604  hdfs dfs -ls /flume01
 1605  ls
 1606  sudo gedit flume-conf1.properties
 1607  cd $HOME
 1608  cd opt
 1609  ls
 1610  cd flume-1.8.0/
 1611  cd conf
 1612  ls
 1613  sudo cp flume-conf1.properties  flume-conf2.properties 
 1614  ls
 1615  sudo gedit flume-conf2.properties
 1616  ls
 1617  sudo gedit flume-conf2.properties
 1618  sudo cp flume-conf2.properties flume-conf3.properties
 1619  sudo gedit flume-conf3.properties
 1620  ls
 1621  mv flume-conf3.properties flume-flume2netcat-example.properties
 1622  ls
 1623  cat flume-conf2.properties
 1624  ls
 1625  la -a
 1626  sudo mv flume-conf3.properties flume2netcat-example.properties
 1627  ls
 1628  cat flume2netcat-example.properties 
 1629  sudo gedit flume2netcat-example.properties 
 1630  ls
 1631  cat flume-conf2.properties
 1632  sudo mv flume-conf2.properties flume2HDFS.properties
 1633  ls
 1634  sudo gedit flume2HDFS.properties 
 1635  ls
 1636  cp flume2*.properties ~/Documents/bigdata_Plumbers
 1637  cd $1
 1638  cd $HOME/Documents/bigdata-Plumbers
 1639  cd $HOME/Documents/bigdata-Plumber
 1640  clear
 1641  ls
 1642  cd Documents
 1643  ls
 1644  cd bigdata_Plumbers
 1645  ls
 1646  mkdir 007_FLUME_CONF_SCRIPTS
 1647  mv flume2*.properties 007_FLUME_CONF_SCRIPTS/
 1648  ls
 1649  . updategit.sh
 1650  . getgit.sh
 1651  . updategit.sh
 1652  cd $HOME
 1653  exit
 1654  curl telnet://localhost:56565
 1655  sudo apt install curl
 1656  curl telnet://localhost:56565 
 1657  history | grep "ncat"
 1658  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties
 1659  flume-ng agent -n agent -C conf -f ~/opt/flume-1.8.0/conf/flume-conf2.properties
 1660  cd opt
 1661  ls
 1662  cd flume-1.8.0/
 1663  cd conf
 1664  ls
 1665  flume-ng agent --conf $FLUME_CONF --conf-file $FLUME_CONF/netcat.conf 
 1666  ls
 1667  sudo gedit flume2netcat-example.properties 
 1668  flume-ng agent -conf $FLUME_CONF --conf-file $FLUME_CONF/flume2netcat_example.properties --Name NetcatAgent -Dflume.root.logger=INFO,CONSOLE
 1669  echo $FLUME_CONF
 1670  export
 1671  cat .bash_profile
 1672  cd opt
 1673  cd sqoop-1.4.7/
 1674  cd conf
 1675  ls
 1676  cat sqoop-env.sh
 1677  cd opt
 1678  cd kafka_2.12-2.0.0/
 1679  pwd
 1680  history | grep "wget" || "sqoop"
 1681  history | grep "wget" && "sqoop"
 1682  ls
 1683  cd ..
 1684  cd $HOME
 1685  ls
 1686  history | grep "wget"
 1687  history | grep sqoop
 1688  history > t001.out
 1689  sudo gedit t001.out
 1690  cd opt
 1691  cd sqoop-1.4.7/
 1692  cd conf
 1693  ls
 1694  sudo gedit sqoop-env.sh
 1695  cd $HOME
 1696  echo $FLUME_HOME
 1697  cd opt
 1698  cd flume-1.8.0/
 1699  cd conf
 1700  ls
 1701  cat flume-env.sh.template
 1702  echo $JAVA_Home
 1703  ls
 1704  mv flume-env.sh.template flume-env.sh
 1705  sudo mv flume-env.sh.template flume-env.sh
 1706  ls
 1707  echo $JAVA_HOME
 1708  echo $PATH
 1709  echo $FLUME_HOME
 1710  cd $HOME
 1711  env > t.out
 1712  gedit t.out
 1713  echo $PATH
 1714  cat .bash_profile
 1715  history > history-2020-07-14-1108.out
 1716  history | grep "wget"
 1717  cd opt
 1718  sudo wget https://archive.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz
 1719  sudo tar -zxf kafka_2.12-2.0.0.tgz 
 1720  ls
 1721  sudo rm -rf kafka_2.12-2.0.0.tgz
 1722  ls
 1723  cd kafka_2.12-2.0.0/
 1724  pwd
 1725  cd ..
 1726  sudo gedit .bash_profile
 1727  source .bash_profile
 1728  echo $PATH
 1729  cd opt/kafka_2.12-2.0.0/
 1730  cd bin
 1731  ls
 1732  cd $HOME
 1733  cd Documents
 1734  cd bigdata_Plumber
 1735  ls
 1736  cd bigdata_Plumbers
 1737  ls
 1738  sudo . getgit.sh
 1739  . getgit.sh
 1740  mv 007_FLUME_CONF_SCRIPTS 007_FLUME_TWITTER
 1741  . updategit.sh
 1742  echo "Task 7: Flume Twitter, (1) Start ingesting data based on keyboard; (2) store it on HDFS" > 007_FLUME_TWITTER/readme.txt
 1743  cat 007_FLUME_TWITTER/readme.txt
 1744  ls
 1745  . updategit.sh
 1746  hdp-select
 1747  cd $HOME
 1748  kafka --version
 1749  kafka-topics.sh
 1750  kafka-topics.sh --version
 1751  history | grep "wget"
 1752  history > t000.out
 1753  sudo gedit t000.out
 1754  cat /etc/hosts
 1755  exit
 1756  cat /etc/hosts
 1757  cat ~/.ssh/authorized_keys
 1758  sqoop version
 1759  history | kafka
 1760  history | grep "kafka"
 1761  cd opt
 1762  ls
 1763  cd ..
 1764  ls
 1765  history > hist001.out
 1766  sudo gedit hist001.out
 1767  exit
 1768  hadoop -version
 1769  hadoop version
 1770  ubuntu version
 1771  cd Documents
 1772  ls
 1773  cd bigdata_Plumbers
 1774  ls
 1775  ls 007_FLUME_TWITTER/
 1776  clear
 1777  ls
 1778  . updategit.sh
 1779  cat updategit.sh
 1780  exit
 1781  cd opt
 1782  cd hadoop-3.1.3/
 1783  cd etc/hadoop
 1784  ls
 1785  cat workers
 1786  exit
 1787  kafka-topics.sh
 1788  kafka-topics.sh --version
 1789  whereis kafka-topics.sh
 1790  cat .bash_profile
 1791  echo $JAVA_HOME
 1792  clear
 1793  tr ":" "\n" <<< "$PATH"
 1794  flume-ng -version
 1795  flume-ng version
 1796  kafka-topics.sh --version
 1797  whereis kafka-topics.sh
 1798  tr ":" "\n" <<< "$PATH"
 1799  echo $JAVA_HOME
 1800  cat .bashrc | more
 1801  cd hadoop-3.1.3/
 1802  ls
 1803  cd etc
 1804  cd hadoop
 1805  ls
 1806  sudo gedit hadoop-env.sh
 1807  exit
 1808  cd $KAFKA_HOME/config
 1809  ls
 1810  source .bash_profile
 1811  zookeeper-server-start.sh $KAFKA_HOME/config
 1812  clear
 1813  zookeeper-server-start.sh \$KAFKA_HOME/config/zookeeper.properties
 1814  jps
 1815  zookeeper -server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
 1816  zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
 1817  more < .bash_profile
 1818  cd $KAFKA_HOME/
 1819  cd
 1820  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1821  ccd opt/
 1822  cd opt/
 1823  ls
 1824  cd kafka_2.12-2.0.0/config/
 1825  ls
 1826  nano zookeeper.properties 
 1827  cd opt
 1828  ls
 1829  rm -rf kafka_2.12-2.0.0/
 1830  sudo rf -rf kafka_2.12-2.0.0/
 1831  sudo rm -rf kafka_2.12-2.0.0/
 1832  ls
 1833  history | grep "sudo kafka"
 1834  history | grep "kafka"
 1835  sudo wget https://archive.apache.org/dist/kafka/2.0.0/kafka_2.12-2.0.0.tgz
 1836  sudo tar -zxf kafka_2.12-2.0.0.tgz 
 1837  ls
 1838  cd kafka_2.12-2.0.0/
 1839  pwd
 1840  cd ..
 1841  sudo gedit .bash_profile
 1842  source .bash_profile
 1843  path
 1844  echo $PATH
 1845  jps
 1846  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1847  cd opt/kafka_2.12-2.0.0/config
 1848  cp server.properties server1.properties
 1849  sudo cp server.properties server1.properties
 1850  ls
 1851  sudo gedit server1.properties 
 1852  cd $HOME
 1853  cd opt
 1854  ls
 1855  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1856  zookeeper-server-start.sh  ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1857  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1858  jps
 1859  cd $HOME
 1860  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1861  jps
 1862  kafka-topics.sh --create --bootstrap-server localhost:9099 --replication-factor 2 partition 1 --topic kafkaTest
 1863  kafka-topics.sh --create --bootstrap-server localhost:9099 --replication-factor 1 partition 1 --topic kafkaTest
 1864  kafka-server-stop.sh
 1865  jps
 1866  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1867  jps
 1868  kafka-topics.sh --zookeeper localhost:9099 --replication-factor 1 --partitions 1 --topic kafkaTest
 1869  kafka-topics.sh --create --zookeeper localhost:9099 --replication-factor 1 --partitions 1 --topic kafkaTest
 1870  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafkaTest
 1871  zookeeper-server-start.sh  ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1872  zookeeper-server-start.sh -daemon ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1873  zookeeper-server-start.sh  ~/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 1874  echo $KAFKA_HOME
 1875  kafka-server-start.sh ~/opt/kafka_2.12-2.0.0/config/server1.properties 
 1876  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafkaTest
 1877  kafka-topics.sh --describe --zookeeper localhost:2181 --topic kafkaTest
 1878  cd opt
 1879  ls
 1880  sudo rm -f kafka_2.12-2.0.0.tgz
 1881  ls
 1882  cd kafka_2.12-2.0.0/config/
 1883  ls
 1884  ls -l
 1885  sudo gedit server1.properties 
 1886  cd $HOME
 1887  jps
 1888  kafka-console-producer.sh --broker-list localhost:9099 --topic kafkaTest
 1889  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafkaTest
 1890  kafka-topics.sh --describe --zookeeper localhost:2181 --topic kafkaTest
 1891  cd opt/kafka_2.12-2.0.0/
 1892  ls
 1893  cd config
 1894  ls
 1895  sudo gedit server1.properties
 1896  cd $HOME
 1897  cd opt/kafka_2.12-2.0.0/cd $HOME
 1898  history | kafka
 1899  history | grep "kafka"
 1900  cd opt
 1901  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties
 1902  cd ..
 1903  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties
 1904  kafka-console-consumer.sh  --bootstrap-server localhost:9099 --topic kafkaTest
 1905  kafka-console-consumer.sh  --bootstrap-server localhost:9099 --topic kafkaTest --from-beginning
 1906  cd Documents
 1907  cd bigdata_Plumber
 1908  cd bigdata_Plumbers
 1909  ls
 1910  . updategit.sh
 1911  cd Documents/bigdata_Plumbers
 1912  ls
 1913  cd 006_Sqoop_and_CodingPractice/
 1914  ls
 1915  more < mapreducetest.java
 1916  ls
 1917  more < mapreducetest.scala
 1918  cd Documents/bigdata_Plumbers
 1919  ls
 1920  . updategit.sh
 1921  ls
 1922  cd 006_Sqoop_and_CodingPractice/
 1923  ls
 1924  more < mapreducetest.java
 1925  ls
 1926  more < mapreducetest.scala
 1927  jps
 1928  python
 1929  pip install pykafka
 1930  python
 1931  pip install kafka-python
 1932  python
 1933  pip install msgpack
 1934  pip install kafka-python
 1935  python
 1936  pip3 install kafka-python
 1937  python
 1938  path
 1939  echo $PYTHON_HOME
 1940  sudo gedit .bash_profile
 1941  source .bash_profile
 1942  whereis python
 1943  cd /usr/bin/python3.8
 1944  sudo cd /usr/bin/python3.8
 1945  ls /usr/bin/python3.8
 1946  cd opt
 1947  ls
 1948  cd Python-3.8.3/
 1949  ls
 1950  ls Modules
 1951  ls
 1952  ls lib
 1953  ls Include
 1954  pwd
 1955  cd $HOME
 1956  sudo edit .bash_profile
 1957  sudo gedit .bash_profile
 1958  source .bash_profile
 1959  path
 1960  echo $PATH | tr ":" "\n"
 1961  pip instlal kafka-python
 1962  pip install kafka-python
 1963  pip3 instlal kafka-python
 1964  pip3 install kafka-python
 1965  python
 1966  echo $PYTHONPATH
 1967  sudo gedit .bash_profile
 1968  source .bash_profile
 1969  pip3 install kafka-python
 1970  python
 1971  cd /home/dev
 1972  path | tr ":" "\n" > t.out
 1973  echo $PATH | tr ":" "\n" > t.out
 1974  l t.out
 1975  more< t.out
 1976  cd /usr/bin
 1977  ls
 1978  ls p*.*
 1979  cd python3.8-config
 1980  more < python3.8-config
 1981  ls
 1982  python
 1983  cd $HOME
 1984  python
 1985  python3.8
 1986  exit
 1987  cd Documents/bigdata_Plumber
 1988  cd Documents/
 1989  cd bigdata_Plumbers/
 1990  cd 006_Sqoop_and_CodingPractice/
 1991  ls
 1992  more < mapreducetest.java
 1993  ls
 1994  more < mapreducetest.scala
 1995  cd $HOME
 1996  ls
 1997  cd Documents
 1998  ls
 1999  cd ..
 2000  cd Downloads
 2001  ls
 2002  cd ..
 2003  pip install kafka-python
 2004  history | grep zookeeper
 2005  history | kafka
 2006  history | grep kafka
 2007  cd opt
 2008  ls
 2009  cd kafka_2.12-2.0.0/
 2010  ls
 2011  cd config
 2012  ls
 2013  cd $HOME
 2014  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2015  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2016  history | grep kafka-topics
 2017  kafka-topics.sh --create --bootstrap-server localhost:9099 --replication-factor 1 partition 1 --topic kafkaTest
 2018  kafka-topics.sh --create --zookeeper localhost:9099 --replication-factor 1 --partitions 1 --topic sample
 2019  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample
 2020  kafka-topics.sh --list --zookeeper localhost:2181
 2021  kafka-topics.sh --describe --zookeeper localhost:2181 --topic sample
 2022  python3.8.3
 2023  cd/usr/bin
 2024  cd /usr/bin
 2025  ls
 2026  ls p*.*
 2027  cd $HOME
 2028  python3.8
 2029  cd opt
 2030  ls
 2031  cd ..
 2032  python3.8
 2033  jps
 2034  python3.8
 2035  history | kafka
 2036  history | grep kafka
 2037  history | grep "localhost:"
 2038  python3.8
 2039  history | grep zookeeper
 2040  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties
 2041  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties
 2042  history | grep localhost
 2043  pip install kafka-python
 2044  pip3 install kafka-python
 2045  pip install python-twitter
 2046  pip3 install python-twitter
 2047  pip install tweepy
 2048  pip3 instlal tweepy
 2049  pip3 install tweepy
 2050  python3.8 testtwitterapi.py
 2051  pip install tweepy
 2052  pip3 install tweepy
 2053  pip install json
 2054  python3.8 testtwitterapi.py
 2055  pip install simplejson
 2056  pip3 install simplejson
 2057  pip install simplejson
 2058  python3.8 testtwitterapi.py
 2059  python3.8
 2060  pip install 'six'
 2061  pip3 install 'six'
 2062  python3.8
 2063  pip uninstall six
 2064  sudo pip uninstall six
 2065  pip uninstall six
 2066  sudo pip uninstall six
 2067  python3.8
 2068  pip install --user six
 2069  pip3 install --user six
 2070  pip instlal --user six.moves
 2071  pip install --user six.moves
 2072  python3.8
 2073  exit
 2074  sudo pip install tweepy
 2075  sudo easy_install pip
 2076  sudo apt-get install python-setuptools
 2077  sudo easy_install pip
 2078  easy_install pip
 2079  pip install tweepy
 2080  pip3 install tweepy
 2081  python3.8
 2082  exit
 2083  sudo jupyter notebook --allow-root
 2084  mkdir jupyter_folder
 2085  jupyter-notebook --notebook-dir jupyter_folder
 2086  audo apt install jupyter-notebook
 2087  sudo apt install jupyter-notebook
 2088  jupyter-notebook --notebook-dir jupyter_folder
 2089  exit
 2090  python
 2091  python3.7
 2092  ls *.py
 2093  gedit test-tweet.py 
 2094  python test-tweet.py 
 2095  ls
 2096  ls *.py
 2097  more < test-tweet.py
 2098  python3.8 test-tweet.py
 2099  jupyter-notebook
 2100  jupyter-notebook
 2101  cd ..
 2102  sudo gedit .bash_profile
 2103  source .bash_profile
 2104  echo $SPARK_HOME
 2105  jps
 2106  kill -help
 2107  kill --help
 2108  kill 48462
 2109  jps
 2110  sudo kill 48462
 2111  jps
 2112  kafka-server-stop.sh -help
 2113  kafka-server-stop.sh --help
 2114  kafka-server-stop.sh 
 2115  jps
 2116  exit() quit()
 2117  exit
 2118  cd opt
 2119  sudo wget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
 2120  ls
 2121  sudo tar zxf spark-2.4.4-bin-hadoop2.7.tgz 
 2122  ls
 2123  mv spark-2.4.4-bin-hadoop2.7 spark-2.4.4
 2124  sudo rm -f  spark-2.4.4-bin-hadoop2.7.tgz 
 2125  ls
 2126  cd ..
 2127  source .bash_profile
 2128  spark-shell
 2129  exit
 2130  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2131  jps
 2132  cd opt/kafka_2.12-2.0.0/config
 2133  ls
 2134  xdg-open
 2135  xdg-open server1.properties
 2136  cd $HOME
 2137  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2138  python3.8
 2139  ls opt
 2140  python3.8
 2141  jps
 2142  kafka-topics.sh --create --zookeeper localhost:9099 --replication-factor 1 --partitions 1 --topic sample
 2143  cd opt/kafka_2.12-2.0.0/config
 2144  ls
 2145  xdg-open zookeeper.properties
 2146  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample
 2147  kafka-topics.sh --list --zookeeper localhost:2181
 2148  kafka-topics.sh --describe --zookeeper localhost:2181 --topic sample
 2149  kafka-server-stop.sh 
 2150  zookeeper-server-stop.sh
 2151  jps
 2152  cd $HOME
 2153  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2154  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2155  python3.8
 2156  jps
 2157  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2158  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample
 2159  kafka-topics.sh --list --zookeeper localhost:2181
 2160  kafka-topics.sh --describe --zookeeper localhost:2181 --topic sample
 2161  jupyter-notebook
 2162  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2163  jps
 2164  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2165  jps
 2166  cd opt/kafka_2.12-2.0.0/config
 2167  sudo gedit server1.properties
 2168  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2169  cd $HOME
 2170  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2171  exit
 2172  ls
 2173  sudo gedit kafka-producer-consumer.py
 2174  ls
 2175  jps
 2176  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2177  jps
 2178  zookeeper-server-stop.sh
 2179  jps
 2180  exit
 2181  jps
 2182  kafka-kafka-server-stop.sh
 2183  jps
 2184  cd opt/kafka_2.12-2.0.0/config
 2185  ls
 2186  sudo gedit server1.properties
 2187  sudo gedit zookeeper.properties
 2188  sudo gedit server1.properties
 2189  cd $HOME
 2190  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2191  jps
 2192  exit
 2193  jps
 2194  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2195  jps
 2196  diff
 2197  cd opt/kafka_2.12-2.0.0/config
 2198  ls
 2199  diff server.properties server1.properties
 2200  sudo gedit server.properties server1.properties[A
 2201  cd $HOME
 2202  jps
 2203  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2204  jps
 2205  zookeeper-server-stop.sh 
 2206  jps
 2207  exit
 2208  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2209  exit
 2210  jps
 2211  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample
 2212  kafka-topics.sh --list --zookeeper localhost:2181
 2213  kafka-topics.sh --describe --zookeeper localhost:2181 --topic sample
 2214  sudo gedit producer1.py
 2215  sudo gedit consumer1.py
 2216  ls *.py
 2217  python3.8 producer1.py
 2218  exit
 2219  python3.8 consumer1.py
 2220  sudo gedit consumer1.py
 2221  python3.8 consumer1.py
 2222  jps
 2223  kafka-server-stop.sh 
 2224  jps
 2225  kafka-server-stop.sh
 2226  jps
 2227  zookeeper-server-stop.sh 
 2228  jps
 2229  jps
 2230  jps
 2231  cd opt/kafka_2.12-2.0.0/config
 2232  xdg-open server1.properties 
 2233  kafka-server-stop.sh 
 2234  jps
 2235  sudo gedit server1.properties 
 2236  cd $HOME
 2237  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2238  jupyter-notebook
 2239  jps
 2240  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2241  jps
 2242  cd opt/kafka_2.12-2.0.0/config
 2243  ls
 2244  cp server.properties server1.properties 
 2245  sudo cp server.properties  server1.properties 
 2246  sudo gedit server1.properties
 2247  cd $HOME
 2248  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2249  jps
 2250  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper 
 2251  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2252  exit
 2253  sudo gedit producer1.py consumer1.py
 2254  ls
 2255  cat threading1.py
 2256  cat threading2.py
 2257  cp threading2.py producer-consumer-threading.py
 2258  sudo gedit producer1.py consumer1.py producer-consumer-threading.py
 2259  python3.8 consumer2.py
 2260  cp producer1.cp producer3.cp
 2261  cp producer1.py producer3.py
 2262  cp consumer1.py consumer3.py
 2263  python3.8 consumer3.py
 2264  python3.8 consumer2.py
 2265  python3.8 consumer3.py
 2266  ls
 2267  python3.8 test-code-000.py
 2268  python3.8 testapi.py
 2269  python3.8 test-code-000.py 
 2270  cp test-code-000.py covid-19-kafka-stream.py
 2271  mv covid-19-kafka-stream.py covid-19-kafka-streaming-pipeline.py
 2272  sudo gedit covid-19-kafka-streaming-pipeline.py 
 2273  cd opt/flume-1.8.0/conf/
 2274  ls
 2275  l flume2HDFS.properties 
 2276  xdg-open flume2HDFS.properties 
 2277  more < flume-conf1.properties
 2278  ls
 2279  cp flume-conf1.properties flumeFromTwitter.properties
 2280  sudo cp flume-conf1.properties flumeFromTwitter.properties
 2281  ls
 2282  sudo gedit flumeFromTwitter.properties 
 2283  cd $HOME
 2284  ls
 2285  more < kafkawithtwitter.py
 2286  ls
 2287  more < testtwitterapi.py
 2288  sudo gedit testtwitterapi.py
 2289  cd Downloads
 2290  ls
 2291  clear
 2292  ls
 2293  ls /w
 2294  ls --help
 2295  man ls
 2296  ls -x
 2297  ls /w
 2298  ls -w
 2299  ls --help | more
 2300  ls -w
 2301  ls --width=0
 2302  ls *.pdf
 2303  mv 'Designing Event-Driven Systems Concepts and Patterns for Streaming Services with Apache Kafka by Ben Stopford (z-lib.org).pdf' 'Designing Event-Driven Systems Concepts and Patterns for Streaming Services with Apache Kafka.pdf' 
 2304  ls *.pdf
 2305  clear
 2306  ls 
 2307  ls *.pdf
 2308  mv 'Learning Spark Lightning-Fast Big Data Analysis by Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia (z-lib.org).pdf' 'Learning Spark Lightning-Fast Big Data Analysis.pdf' 
 2309  ls *.pdf
 2310  mv 'Practical Real-time Data Processing and Analytics Distributed Computing and Event Processing using Apache Spark, Flink, Storm, and Kafka by Shilpi Saxena, Saurabh Gupta (z-lib.org).pdf' 'Practical Real-time Data Processing and Analytics Distributed Computing and Event Processing using Apache Spark, Flink, Storm, and Kafka.pdf' 
 2311  ls *.epub
 2312  mv 'Learning PySpark by Denny Lee, Tomasz Drabas (z-lib.org).epub'  'Learning PySpark.epub' 
 2313  ls *.epub
 2314  mv 'PySpark Cookbook Over 60 Recipes for Implementing Big Data Processing and Analytics Using Apache Spark and Python by Tomasz Drabas Denny Lee (z-lib.org).epub' 'PySpark Cookbook Over 60 Recipes for Implementing Big Data Processing and Analytics Using Apache Spark and Python.epub' 
 2315  ls *.epub
 2316  ls *.pdf
 2317  clear
 2318  ls *.pdf
 2319  ls *.peub
 2320  ls *.epub
 2321  sudo apt install fbreader
 2322  sudo add-apt-repository ppa:bookworm-team/bookworm
 2323  sudo apt update
 2324  sudo apt install bookworm
 2325  snap install bookworm
 2326  snap install --candidate bookworm
 2327  snap install --beta bookworm
 2328  snap install --edge bookworm
 2329  snap install --devmode --edge bookworm
 2330  cd $HOME
 2331  cd opt
 2332  history | grep "get"
 2333  sudo wget https://archive.apache.org/dist/nifi/1.9.0/nifi-1.9.0-bin.tar.gz
 2334  mv ../Downloads/nifi-1.9.0-bin.tar.gz .
 2335  ls
 2336  ls -l
 2337  ls ../Downloads
 2338  ls -l ../Downloads/nifi-1.9.0-bin.tar.gz 
 2339  cp ../Downloads/nifi-1.9.0-bin.tar.gz nifi-1.9.0-bin.tar.gz 
 2340  ls
 2341  history | grep tar
 2342  sudo tar -zxf nifi-1.9.0-bin.tar.gz 
 2343  ls
 2344  cp ../Downloads/nifi-1.9.0-bin.tar.gz nifi.tar.gz
 2345  sudo tar -zxf nifi.tar.gz
 2346  ls
 2347  rm -rf nifi-1.9.0-bin.tar.gz
 2348  ls
 2349  cd nifi-1.9.0
 2350  ls
 2351  ls conf
 2352  cd ..
 2353  ls
 2354  rm -f nifi.tar.gz 
 2355  ls
 2356  cd nifi-1.9.0/
 2357  pwd
 2358  cd $HOME
 2359  sudo gedit .bash_profile
 2360  source .bash_profile
 2361  echo $NIFI_HOME
 2362  cd Downloads
 2363  ls nifi*.*
 2364  rm -f nifi-1.9.0-bin.tar.gz 
 2365  ls
 2366  cd ..
 2367  ls *.py
 2368  more < covid-19-kafka-streaming-pipeline.py 
 2369  cp covid-19-kafka-streaming-pipeline.py covid-19-kafka-producer.py 
 2370  sudo gedit covid-19-kafka-producer.py
 2371  cd Downloads
 2372  ls
 2373  ls | more
 2374  cd /usr/lib
 2375  ls
 2376  cd hive
 2377  cd $HOME
 2378  cd opt
 2379  cd hive-2.3.5/
 2380  ls
 2381  cd metastore_db
 2382  ls
 2383  cd $HOME
 2384  ls
 2385  cd opt/hive-2.3.5/
 2386  ls
 2387  cd lib
 2388  mkdir metastore_db
 2389  sudo mkdir metastore_db
 2390  ls
 2391  history | grep chmod
 2392  history | grep metastore
 2393  history | grep metastore_db
 2394  pwd
 2395  cd ..
 2396  pwd
 2397  cd conf
 2398  ls
 2399  cp hive-site.xml hive-site.xml.bak
 2400  sudo cp hive-site.xml hive-site.xml.bak
 2401  ls
 2402  sudo gedit hive-site.xml
 2403  cd ..
 2404  ls
 2405  mkdir metastore_db
 2406  cd metastore_db
 2407  ls
 2408  cd ..
 2409  hive --service metastore
 2410  cd $HOME
 2411  hive --service metastore
 2412  source .bash_profile
 2413  hive --service metastore
 2414  cd opt/hive-2.3.5/
 2415  ls
 2416  ls metastore_db
 2417  ls
 2418  mv metastore_db metastore_db.bak
 2419  sudo mv metastore_db metastore_db.bak
 2420  ls
 2421  cd $HOME
 2422  hive --service metastore
 2423  cd opt
 2424  ls
 2425  sudo chmod -R a+rwx hive-2.3.5/
 2426  cd ..
 2427  hive --service metastore
 2428  hive --service hiveserver
 2429  hive
 2430  exit
 2431  cd Documents
 2432  cd bigdata_Plumber
 2433  cd bigdata_Plumbers
 2434  ls
 2435  mkdir 999
 2436  echo "BACKUP/MISC" > 999/readme.txt
 2437  cd 999
 2438  copy $HOME\*.py .
 2439  lscp $HOME/*.py
 2440  cp $HOME/*.py
 2441  cp $HOME/*.py .
 2442  ls
 2443  cd ..
 2444  ls
 2445  . updategit.sh
 2446  python3.8 threading1.py
 2447  ls *.py
 2448  cd $HOME
 2449  python3.8 threading1.yp
 2450  python3.8 threading.py
 2451  python3.8 threading1.py
 2452  python3.8 threading2.py
 2453  cd Documents/bigdata_Plumbers/
 2454  ls
 2455  cd 999
 2456  ls
 2457  copy ~/*.py
 2458  cp ~/*.py .
 2459  ls
 2460  cd ..
 2461  . updategit.sh
 2462  cd $HOME
 2463  ls
 2464  ls *.py
 2465  xdg-open test.py
 2466  jupyter-notebook
 2467  exit
 2468  jps
 2469  ls *.py
 2470  python3.8 test-covid-19-api.py 
 2471  pip install requests
 2472  pip3 install requests
 2473  python3.8 test-covid-19-api.py 
 2474  cd opt
 2475  ls
 2476  cd Python-3.8.3/
 2477  ls
 2478  cd modules
 2479  ls
 2480  cd Tools
 2481  ls
 2482  cd scripts
 2483  pip install requests
 2484  pip3 install requests
 2485  cd $HOME
 2486  python3.8 test-covid-19-api.py 
 2487  sudo pip install requests
 2488  pip install requests
 2489  pip3 install requests
 2490  sudo apt-get python-requests
 2491  sudo apt-get install python-requests
 2492  sudo apt-get install python3-requests
 2493  python3.8 test-covid-19-api.py 
 2494  sudo pip install requests
 2495  whereis pip
 2496  sudo /usr/local/bin/pip3.8 install requests
 2497  python3.8 testtwitterapi.py
 2498  python3.8 test-covid-19-api.py 
 2499  python3.8 test-covid-19-api.py > testapi.out
 2500  more < testapi.out
 2501  grep "United" testapi.out
 2502  grep "USA" testapi.out
 2503  cat testapi.out | tr "}," "\n" 
 2504  cat testapi.out | tr "}," "}\n" 
 2505  cat testapi.out | tr "}," "}\n" | tr "{" "\n{"
 2506  cat testapi.out | tr "}," "}\n" | tr "{" "\n{" > tt.out
 2507  xdg-open tt.out
 2508  ls
 2509  jps
 2510  cp test-covid-19-api.py testapi.py
 2511  man alias
 2512  sudo gedit .bash_aliases
 2513  source .bash_aliases
 2514  ls
 2515  l testapi.py
 2516  cd Documents/bigdata_Plumbers
 2517  ls
 2518  cd 999
 2519  ls
 2520  sudo gedit testtwitterapi.py
 2521  sudo gedit test-tweet.py
 2522  sudo gedit twitter-streaming-test.py
 2523  sudo gedit twitter-test.py
 2524  sudo gedit kafkawithtwitter.py
 2525  cd ..
 2526  . updategit.sh
 2527  cd $HOME
 2528  ls
 2529  sudo gedit producer1.py consumer1.py 
 2530  python3.8 producer-consumer-threading.py 
 2531  cat threading1.py
 2532  python3.8 producer-consumer-threading.py 
 2533  python3.8 producer1.py
 2534  python3.8 consumer1.py
 2535  python3.8 producer-consumer-threading.py 
 2536  python3.8 consumer1.py
 2537  cat producer1.py
 2538  cat producer2.py
 2539  cat consumer1.py
 2540  python3.8 producer2.py
 2541  cat producer3.py
 2542  python3.8 producer3.py
 2543  python3.8 producer2.py
 2544  python3.8 producer3.py
 2545  sudo gedit test.py
 2546  sudo gedit test-code-000.py
 2547  ls
 2548  python3.8 test-code-000.py
 2549  python3.8 test-code-000.py > t.out
 2550  l t.out
 2551  python3.8 test-code-000.py > t.out
 2552  l t.out
 2553  ls
 2554  cat testapi.py
 2555  sudo gedit testapi.y
 2556  sudo gedit testapi.py
 2557  python3.8 testapi.py
 2558  cp producer3.py producer4.py
 2559  cp consumer3.py consumer4.py
 2560  ls
 2561  sudo gedit producer4.py consumer4.py test-code-000.py testapi.py
 2562  cd opt/flume-1.8.0/conf/
 2563  ls
 2564  cat flume-twitter-example.properties
 2565  ls
 2566  sudo gedit flumeTwitter2HDFS.properties flume-twitter-example.properties flume2HDFS.properties 
 2567  cd $HOME
 2568  python3.8 forever-kafka-consumer.py 
 2569  ls *.txt
 2570  cat t1.txt
 2571  cat test.txt
 2572  pwd
 2573  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 forever-kafka-consumer.py 
 2574  cd opt
 2575  sudo wget https://archive.apache.org/dist/hive/hive-1.2.0/apache-hive-1.2.0-bin.tar.gz
 2576  history | grep tar
 2577  sudo tar -zxf apache-hive-1.2.0-bin.tar.gz 
 2578  ls
 2579  rm -f apache-hive-1.2.0-bin.tar.gz 
 2580  ls
 2581  cd apache-hive-1.2.0-bin/
 2582  ls
 2583  pwd
 2584  cd $HOME
 2585  sudo gedit .bash_profile
 2586  source .bash_profile
 2587  sudo gedit .bash_profile
 2588  sudo gedit .bashrc
 2589  cd opt
 2590  ls
 2591  cd apache-hive-1.2.0-bin/
 2592  ls
 2593  cd bin
 2594  ls
 2595  sudo gedit hive-config.sh 
 2596  cd $HOME
 2597  cd opt
 2598  ls
 2599  cd hadoop-3.1.3/
 2600  pwd
 2601  cd $HOME
 2602  hdfs dfs -mkdir /tmp
 2603  history | grep "startall"
 2604  start-all.sh
 2605  jps
 2606  hdfs dfs -mkdir /tmp
 2607  hdfs dfs -chmod g+w /tmp
 2608  hdfs dfs -ls /
 2609  hdfs dfs -mkdir -p /user/hive/warehouse
 2610  hdfs dfs -chmod g+w /user/hive/warehouse
 2611  hdfs dfs -ls /user/hive
 2612  cd opt
 2613  cd apache-hive-1.2.0-bin/
 2614  cd conf
 2615  ls
 2616  cp hive-default.xml.template hive-site.xml
 2617  sudo cp hive-default.xml.template hive-site.xml
 2618  ls
 2619  sudo gedit hive-site.xml 
 2620  cd $HOME
 2621  clear
 2622  cd opt
 2623  ls
 2624  rm -rf apache-hive-1.2.0-bin/
 2625  ls
 2626  sudo rm -rf apache-hive-1.2.0-bin/
 2627  ls
 2628  clear
 2629  sudo wget https://archive.apache.org/dist/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz
 2630  ls
 2631  history | grep "tar"
 2632  sudo tar zxf apache-hive-2.3.5-bin.tar.gz 
 2633  mv apache-hive-2.3.5-bin hive-2.3.5
 2634  ls
 2635  sudo rm -rf apache-hive-2.3.5-bin.tar.gz 
 2636  ls
 2637  cd hive-2.3.5/
 2638  pwd
 2639  cd $HOME
 2640  sudo gedit .bash_profile
 2641  source .bash_profile
 2642  echo $HIVE_HOME
 2643  cd opt/hive-2.3.5/
 2644  cd conf
 2645  ls
 2646  cd $HOME
 2647  hive
 2648  cd opt/hive-2.3.5/conf/
 2649  cd ../bin
 2650  ls
 2651  cd ../conf
 2652  ls
 2653  sudo gedit hive-site.xml
 2654  cd $HOME
 2655  xdg-open .bash_profile
 2656  cd opt/hive-2.3.5/conf/
 2657  pwd
 2658  ls
 2659  cat hive-site.xml
 2660  cp hive-env.sh.template  hive-env.sh
 2661  sudo cp hive-env.sh.template hive-env.sh
 2662  sudo gedit hive-env.sh
 2663  hdfs dfs -mkdir /user/hive/warehouse
 2664  hdfs dfs -mkdir -p /user/hive/warehouse
 2665  hdfs dfs -mkdir -p /temp
 2666  hdfs dfs -ls /usr/hive
 2667  hdfs dfs -ls /user/hive
 2668  hdfs dfs -chmod g+w /user/hive/warehouse
 2669  hdfs dfs -chmod g+w /temp
 2670  cd $HOME
 2671  cd opt
 2672  cd hive-2.3.5/
 2673  mkdir metastore_db
 2674  sudo mkdir metastore_db
 2675  cd conf
 2676  ls
 2677  sudo gedit hive-site.xml
 2678  sudo gedit hive-env.sh
 2679  cd ..
 2680  ls
 2681  cd metastore_db/
 2682  pwd
 2683  cd ../conf
 2684  sudo gedit hive-env.sh
 2685  ls
 2686  xdg-open hive-env.sh.template
 2687  cd ..
 2688  ls
 2689  mv lib/log4j-slf4j-impl-2.6.2.jar lib/log4j-slf4j-impl-2.6.2.jar.bak
 2690  sudo mv lib/log4j-slf4j-impl-2.6.2.jar lib/log4j-slf4j-impl-2.6.2.jar.bak
 2691  ls
 2692  ls lib
 2693  ls lib/log4j-slf4j-impl-2.6.2.jar.bak
 2694  cd $HOME
 2695  schematool -initSchema -dbType derby 
 2696  hive --version
 2697  cd opt/hive-2.3.5/
 2698  cd bin
 2699  schematool -initSchema -dbType derby 
 2700  jps
 2701  cd ../conf
 2702  java --version
 2703  java -versoin
 2704  java -version
 2705  where java
 2706  whereis java
 2707  cd /usr/bin/java
 2708  cd /usr/bin
 2709  ls
 2710  ls java*.*
 2711  ls
 2712  cd $HOME
 2713  whereis java
 2714  cd opt
 2715  ls
 2716  cd hive-2.3.5/
 2717  cd lib
 2718  ls gua*.*
 2719  mv guava-14.0.1.jar guava-14.0.1.jar.bak
 2720  sudo mv guava-14.0.1.jar guava-14.0.1.jar 
 2721  sudo mv guava-14.0.1.jar guava-14.0.1.jar.bak 
 2722  ls gua*.*
 2723  cd ..
 2724  cd $HOME
 2725  schematool -initSchema -dbType derby 
 2726  cd opt
 2727  ls
 2728  cd hadoop-3.1.3/
 2729  cd lib
 2730  ls gua*.*
 2731  cd ../hdfs
 2732  ls gua*.*
 2733  ls
 2734  cd $HOME/opt
 2735  cd hive-2.3.5/
 2736  cd lib
 2737  ls
 2738  ls gua*.*
 2739  cd $HOME
 2740  clear
 2741  mkdir metastore_db
 2742  ls
 2743  cd metastore_db/
 2744  pwd
 2745  cd ../opt/hive-2.3.5/conf
 2746  sudo gedit hive-site.xml 
 2747  cd $HOME
 2748  history
 2749  schematool -initSchema -dbType derby 
 2750  cd opt/hadoop-3.1.3/lib
 2751  ls gua*.*
 2752  cd ~/opt/hadoop-3.1.3/share/hadoop/hdfs/lib
 2753  cp guava-27.0-jre.jar ~/opt/hive-2.3.5/lib/
 2754  sudo cp guava-27.0-jre.jar ~/opt/hive-2.3.5/lib/
 2755  cd $HOME
 2756  schematool -initSchema -dbType derby 
 2757  cd metastore_db
 2758  chmod 777 .
 2759  ls -l
 2760  cd ..
 2761  schematool -initSchema -dbType derby 
 2762  rm -rf metastore_db/
 2763  ls
 2764  schematool -initSchema -dbType derby 
 2765  hadoop --version
 2766  hadoop version
 2767  cd opt/hive-2.3.5/bin
 2768  ls
 2769  ls schematool
 2770  cd schematool
 2771  ls -l
 2772  cd $HOME
 2773  hive
 2774  schematool -Hive --service metastore_db/
 2775  hive
 2776  hive --service metastore
 2777  hive --service metastore_db
 2778  hive --service metastore
 2779  cd opt/hive-2.3.5/
 2780  ls
 2781  cat conf
 2782  cd conf
 2783  ls
 2784  xdg-open hive-env.sh
 2785  cat hive-site.xml
 2786  history | grep "hive-env"
 2787  ls
 2788  sudo gedit hive-env.sh
 2789  . hive-env.sh
 2790  xdg-open hive-site.xml
 2791  more < hive-site.xml
 2792  sudo gedit hive-site.xml
 2793  python3.8 consumer1.py 
 2794  python3.8 producer1.py
 2795  ls
 2796  more << tt.out
 2797  more < tt.out
 2798  more < t.out
 2799  ls
 2800  more < testapi.out
 2801  cp producer1.py producer-string.py
 2802  cp consumer1.py consumer-string.py
 2803  l testapi.out
 2804  l
 2805  ls
 2806  cat .bash_aliases
 2807  source .bash_aliases
 2808  l testapi.out
 2809  sudo gedit producer2.py consumer2.py
 2810  sudo gedit producer2.py producer3.py consumer2.py consumer3.py
 2811  cd Documents/bigdata_Plumbers
 2812  ls
 2813  cd 007_FLUME_TWITTER/
 2814  ls
 2815  more < flume2HDFS.properties 
 2816  cd ~/opt/flume-1.8.0/config
 2817  cd ~/opt/flume-1.8.0/conf/
 2818  ls
 2819  sudo gedit flume-twitter-example.properties
 2820  rm -f flumeFromTwitter.properties 
 2821  sudo rm -f flumeFromTwitter.properties 
 2822  cp flume2HDFS.properties flumeTwitter2HDFS.properties
 2823  sudo cp flume2HDFS.properties flumeTwitter2HDFS.properties
 2824  sudo gedit flume2HDFS.properties flumeFromTwitter.propties flumeTwitter2HDFS.properties 
 2825  sudo gedit flume2HDFS.properties flumeFromTwitter.properties flumeTwitter2HDFS.properties 
 2826  cd $HOME
 2827  ls
 2828  ls -l *.py
 2829  python3.8 covid-19-kafka-producer.py
 2830  jps
 2831  python3.8 covid-19-kafka-producer.py
 2832  ls -l *.py
 2833  cat producer1.py
 2834  cat consumer1.py
 2835  cp producer1.py forever-kafka-producer.py
 2836  sudo gedit forever-kafka-producer.py 
 2837  ls -l *.py
 2838  cp consumer1.py forever-kafka-consumer.py
 2839  sudo gedit forever-kafka-consumer.py
 2840  python3.8 forever-kafka-producer.py 
 2841  pyspark
 2842  pyspark --master local[4]
 2843  ls -l
 2844  mv output output.txt
 2845  cat output.txt
 2846  cd output.txt
 2847  ls
 2848  cat part-0000
 2849  cat part-00000
 2850  cd ..
 2851  mv output.txt output
 2852  ls
 2853  echo 'print("Hello, World!")' > hello-world.py
 2854  cat hello-world.py
 2855  spark-submit hello-world.py 
 2856  python
 2857  ls
 2858  ls -l *.py
 2859  python forever-kafka-producer.py
 2860  python3.8 forever-kafka-producer.py
 2861  ls -l *.py
 2862  cat testapi.py
 2863  python3.8 testapi.py
 2864  more < kafka-with-covid-19-api.py 
 2865  xdg-open kafka-with-covid-19-api.py
 2866  ls *.py
 2867  more < test-covid-19-api.py
 2868  python3.8 test-covid-19-api.py
 2869  sudo gedit  test-covid-19-api.py
 2870  ls *.py
 2871  more covid-19-kafka-streaming-pipeline.py 
 2872  sudo gedit  covid-19-kafka-streaming-pipeline.py 
 2873  python3.8 forever-kafka-producer.py
 2874  clear
 2875  python3.8 forever-kafka-producer.py
 2876  sudo gedit json-format.out
 2877  more < json-format.out
 2878  ls *.py
 2879  python3.8 producer1.py
 2880  exit
 2881  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2882  cd opt/kafka_2.12-2.0.0/config
 2883  ls
 2884  xdg-open server1.properties 
 2885  cd $HOME
 2886  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 2887  exit
 2888  python3.8 consumer1.py
 2889  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 forever-kafka-consumer.py
 2890  cp forever-kafka-consumer.py kafka2spark-consumer.py
 2891  sudo gedit kafka2spark-consumer.py
 2892  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py 
 2893  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py > t.out
 2894  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py >& t.out
 2895  xdg-open t.out
 2896  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py 
 2897  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py
 2898  python3.8 read-api-reformat.py 
 2899  python3.8 read-api-reformat.py > t.out
 2900  more < t.out
 2901  python3.8 read-api-reformat.py > t.out
 2902  more < t.out
 2903  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py
 2904  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample
 2905  kafka-topics.sh --list --zookeeper localhost:2181
 2906  kafka-topics.sh --describe --zookeeper localhost:2181 --topic sample
 2907  cat producer1.py
 2908  python3.8 producer1.py
 2909  ls *.py
 2910  cp forever-kafka-producer.py kafka2spark-producer.py
 2911  sudo gedit kafka2spark-producer.py 
 2912  python3.8 kafka2spark-producer.py 
 2913  python3.8 read-api-reformat.py 
 2914  python3.8 read-api-reformat.py > t.out
 2915  xdg-open t.out
 2916  python3.8 read-api-reformat.py > t.out
 2917  xdg-open t.out
 2918  more < t.out
 2919  python3.8 read-api-reformat.py > t.out
 2920  more < t.out
 2921  python3.8 read-api-reformat.py > t.out
 2922  more < t.out
 2923  python3.8 read-api-reformat.py > t.out
 2924  more < t.out
 2925  python3.8 kafka2spark-producer.py 
 2926  du
 2927  df -h
 2928  df -BG
 2929  hive
 2930  cd ..
 2931  ls
 2932  cd $HOME
 2933  ls
 2934  ls -l
 2935  cp testapi.out t.out
 2936  sudo gedit t.out
 2937  jps
 2938  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample
 2939  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 2940  cd $HOME
 2941  sudo gedit .bash_profile
 2942  cd opt/hadoop-2.8.0/
 2943  etc/hadoop
 2944  sudo gedit core-site.xml
 2945  cd etc
 2946  cd hadoop
 2947  ls core-site.xml
 2948  sudo gedit core-site.xml
 2949  ls hdfs-site.xml
 2950  sudo gedit hdfs-site.xml
 2951  cd ../..
 2952  ls
 2953  mkdir hadoopdata
 2954  cd hadoopdata
 2955  cd hdfs
 2956  mkdir hdfs
 2957  cd hdfs
 2958  mkdir namenode
 2959  mkdir datanode
 2960  pwd
 2961  cd $HOME
 2962  cd opt
 2963  cd hadoop-2.8.0/
 2964  cd etc/hadoop/
 2965  sudo gedit mapred-site.xml
 2966  sudo gedit yarn-site.xml
 2967  cd $HOME
 2968  source .bash_profile
 2969  hdfs namenode -format
 2970  cd $HADOOP_HOME
 2971  cd sbin
 2972  ls start-dfs.sh
 2973  . start-dfs.sh
 2974  ls start-yran.sh
 2975  ls start-yarn.sh
 2976  . start-yarn.sh
 2977  ls yarn_daemon*.*
 2978  ls
 2979  pwd
 2980  more < yarn_daemon.sh
 2981  more < start-yarn.sh
 2982  cd ..
 2983  cd etc/hadoop
 2984  ls
 2985  more hadoop-env.sh
 2986  xdg-open start-yarn.sh
 2987  ls start-yarn.sh
 2988  ls
 2989  cat yarn-env.sh
 2990  xdg-open yarn-env.sh
 2991  cd $HOME
 2992  ms gedit .bash_profile
 2993  sudo gedit .bash_profile
 2994  source .bash_profile
 2995  echo $PATH | tr ":" "\n" | more
 2996  echo $PATH | tr ":" "\n" | grep "hadoop"
 2997  cd opt/hadoop-2.8.0/etc/hadoop
 2998  xdg-open hadoop-env.sh
 2999  pwd
 3000  ls start-yarn.sh
 3001  cd ..
 3002  cd sbin
 3003  ls start-yarn.sh
 3004  cd $HOME
 3005  cd opt/hadoop-2.8.0/etc/hadoop
 3006  . start-yarn.sh
 3007  cd ..
 3008  cd sbin
 3009  sudo gedit start-yarn.sh
 3010  cd ../etc/hadoop
 3011  ./start-yarn.sh
 3012  . start-yarn.sh
 3013  echo "test" > test.txt
 3014  mv test.txt $HOME
 3015  hdfs dfs -put test.txt /
 3016  hdfs dfs -ls /
 3017  history | grep "hdfs"
 3018  hdfs dfs -mkdir /tmp
 3019  hdfs dfs -put "test.txt" /test
 3020  cd $HOME
 3021  ls test.txt
 3022  cat test.txt
 3023  hdfs dfs -put test.txt /
 3024  hdfs dfs -put test.txt /tmp
 3025  hdfs dfs -mkdir /test
 3026  hdfs dfs -ls /
 3027  hdfs namenode -format
 3028  cd $HADOOP_HOME/sbin
 3029  ./start-dfs.sh
 3030  ./start-yarn.sh
 3031  hdfs dfs -mkdir /user
 3032  hdfs dfs -mkdir /user/hadoop
 3033  hdfs dfs -ls /
 3034  hdfs dfs -ls /user
 3035  hdfs dfs -get logs /tmp/logs
 3036  hdfs dfs -mkdir /user
 3037  jps
 3038  start-all.sh
 3039  jps
 3040  start-dfs.sh
 3041  start-yarn.sh
 3042  jps
 3043  hdfs namenode -format
 3044  start-all.sh
 3045  jps
 3046  start-dfs.sh
 3047  cd ..
 3048  cd hadoopdata
 3049  ls
 3050  cd hdfs
 3051  ls
 3052  ls datanode
 3053  ls namenode
 3054  jps
 3055  ls
 3056  ls datanode
 3057  ls namenode
 3058  hdfs namenode -format
 3059  cd ../../sbin
 3060  start-dfs.sh
 3061  start-yarn.sh
 3062  jps
 3063  cd $HOME
 3064  hdfs dfs -mkdir /user
 3065  hdfs namenode -format
 3066  cd opt
 3067  cd hadoop-2.8.0/sbin
 3068  start-dfs.sh
 3069  export HADOOP_CONF_DIR = $HADOOP_HOME/etc/hadoop
 3070  set HADOOP_CONF_DIR = $HADOOP_HOME/etc/hadoop
 3071  echo $HADOOP_CONF_DIR
 3072  hdfs namenode -format
 3073  hdfs -namenodes
 3074  hdfds getconf -namenodes
 3075  hdfs getconf -namenodes
 3076  start-dfs.sh
 3077  start-yarn.sh
 3078  jps
 3079  cd $HOME
 3080  source .bash_profile
 3081  cd hdfs namenode -format
 3082  hdfs namenode -format
 3083  cd /etc
 3084  cat hosts
 3085  cd $HOME
 3086  cd opt/
 3087  cd hadoop-2.8.0/etc/hadoop
 3088  hdfs namenode -format
 3089  cd ../../sbin
 3090  start-dfs.sh
 3091  start-yarn.sh
 3092  jps
 3093  hdfs -dfs -mkdir /user
 3094  hdfs dfs -mkdir /user
 3095  hdfs dfs -ls 
 3096  hdfs dfs -ls /
 3097  jps
 3098  hdfs -dfs -ls /
 3099  hdfs dfs -ls /
 3100  jps
 3101  stopall.sh
 3102  stop-all.sh
 3103  cd ..
 3104  cd hadoopdata
 3105  cd hdfs
 3106  ls
 3107  ls datanode
 3108  ls namenode
 3109  cd ..
 3110  cd $HOME
 3111  hdfs namenode
 3112  hdfs namenode -format
 3113  start-all.sh
 3114  jps
 3115  cd opt/hadoop-2.8.0/
 3116  cd hadoopdata
 3117  ls
 3118  cd hdfs
 3119  ls
 3120  sudo rm -rf datanode namenode
 3121  cd $HOME
 3122  hdfs namenode -format
 3123  start-all.sh
 3124  jps
 3125  stop-all.sh
 3126  jps
 3127  cd opt/hadoop-2.8.0/
 3128  cd etc/hadoop
 3129  cd $HOME
 3130  cd opt/hadoop-2.8.0/
 3131  ls
 3132  cd hadoopdata
 3133  ls
 3134  cd hdfs
 3135  ls
 3136  rm -rf datanode namenode
 3137  ls
 3138  cd $HOME
 3139  ls
 3140  hdfs namenode -format
 3141  jps
 3142  start-dfs.sh
 3143  jps
 3144  start-yarn.sh
 3145  jps
 3146  start-all.sh
 3147  jps
 3148  stop-all.sh
 3149  start-all.sh
 3150  jps
 3151  sudo gedit .bash_profile
 3152  cd opt/
 3153  ls
 3154  cd hadoop-2.8.0/
 3155  cd etc/hadoop/
 3156  ls
 3157  sudo gedit core-site.xml
 3158  sudo gedit hadoop-env.sh
 3159  sudo gedit hdfs-site.xml
 3160  cd
 3161  source .bash_profile
 3162  hdfs namenode -formate
 3163  hdfs namenode -format
 3164  start-all.sh 
 3165  jps
 3166  cd opt/hadoop-2.8.0/
 3167  ls
 3168  rm -rf hadoopdata/
 3169  ls
 3170  mdkir hdfs 
 3171  mkdir hdfs 
 3172  cd hdfs/
 3173  mkdir namenode
 3174  mkdir datanode
 3175  pwd
 3176  cd ..
 3177  cd etc/hadoop/ 
 3178  sudo gedit hdfs-site.xml
 3179  cd
 3180  hdfs namenode -format
 3181  start-all.sh 
 3182  jps
 3183  stop-all.sh 
 3184  jps
 3185  sudo gedit .bash_profile
 3186  hadoop versio
 3187  cd opt/hadoop-2.8.0/etc/hadoop/
 3188  sudo gedit yarn-site.xml
 3189  sudo gedit mapred-site.xml
 3190  sudo gedit hdfs-site.xml
 3191  cd
 3192  hdfs namenode -format
 3193  stop-all.sh 
 3194  start-all.sh 
 3195  cd opt/hadoop-2.8.0/etc/hadoop/
 3196  sudo gedit core-site.xml
 3197  sudo gedit hdfs-site.xml
 3198  cd
 3199  sudo .gedit .bash_profile
 3200  sudo gedit .bash_profile
 3201  cd opt/hadoop-2.8.0/
 3202  ls
 3203  cd
 3204  sudo gedit .bash_profile
 3205  source .bash_profile
 3206  hdfs namenode -format
 3207  start-all.sh 
 3208  jps
 3209  cd opt/hadoop-2.8.0/
 3210  mkdir hdfs
 3211  cd hdfs/
 3212  mkdir datanode
 3213  mkdir namenode
 3214  cd ..
 3215  cd etc/hadoop/
 3216  cd .. 
 3217  cd ..
 3218  pwd
 3219  cd hdfs/
 3220  pwd
 3221  cd ..
 3222  cd etc/hadoop/
 3223  sudo gedit hdfs-site.xml
 3224  cd
 3225  rm -rf hdfs/
 3226  hdfs namenode -formate
 3227  cd opt/hadoop-2.8.0/etc/hadoop/
 3228  sudo gedit core-site.xml
 3229  cd
 3230  hdfs namenode -formate
 3231  start-all.sh 
 3232  jps
 3233  ls
 3234  history | schematool
 3235  history | grep "schematool"
 3236  schematool -initSchema -dbType derby
 3237  cd opt
 3238  cd hive-2.3.5/
 3239  ls
 3240  clear
 3241  ls
 3242  cd conf
 3243  ls
 3244  sudo gedit hive-env.sh
 3245  cd $HOME
 3246  ls *.out
 3247  xdg-open testapi.out
 3248  ls *.py
 3249  cat testapi.py
 3250  python3.8 testapi.py
 3251  cp testapi.py read-api-reformat.py
 3252  sudo gedit read-api-reformat.py 
 3253  python3.8 read-api-reformat.py 
 3254  python3.8 read-api-reformat.py 
 3255  ls *.py
 3256  cat producer-consumer-threading.py
 3257  ls *.out
 3258  history > hist002.out
 3259  xdg-open hist002.out
 3260  cat producer1.py
 3261  zookeeper-server-start.sh opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 3262  cd opt
 3263  ls
 3264  sudo rm -rf hadoop-3.1.3/
 3265  sudo wget https://archive.apache.org/dist/hadoop/hadoop-2.8.0.tar.gz
 3266  ls
 3267  history | grep "tar"
 3268  tar zxf hadoop-2.8.0.tar.gz 
 3269  ls
 3270  sudo rm -rf hadoop-2.8.0.tar.gz 
 3271  ls
 3272  cd hadoop-2.8.0/
 3273  pwd
 3274  cd $HOME
 3275  sudo gedit .bash_profile
 3276  cd opt/hadoop-2.8.0/
 3277  cd etc
 3278  ls
 3279  cd hadoop
 3280  ls
 3281  sudo cp hadoop-env.sh hadoop-env.sh.bak
 3282  sudo cp core-site.xml core-site.xml.bak
 3283  sudo cp hdfs-site.xml hdfs-site.xml.bak
 3284  sudo cp mapred-site.xml mapred-site.xml.bak
 3285  ls map*.*
 3286  sudo cp mapred-site.xml.template mapred-site.xml.bak
 3287  sudo cp yarn-site.xml yarn-site.xml.bak
 3288  sudo cp mapred-site.xml.template mapred-site.xml
 3289  sudo gedit hadoop-env.sh
 3290  pwd
 3291  cd
 3292  mkdir hdfs
 3293  cd hdfs/
 3294  mkdir namenode
 3295  mkdir datanode
 3296  pwd
 3297  cd
 3298  jps
 3299  kill -9 28086
 3300  jps
 3301  clear
 3302  hdfs namenode -format
 3303  source .bash_profile
 3304  hdfs namenode -format
 3305  cd opt/hadoop-2.8.0/
 3306  rm -rf hdfs/
 3307  cxd
 3308  cd
 3309  hdfs namenode -format
 3310  cd opt/hadoop-2.8.0/etc/hadoop/
 3311  ls
 3312  sudo gedit hdfs-site.xml
 3313  cd
 3314  hdfs namenode -format
 3315  cd hdfs/
 3316  ls
 3317  cd namenode/
 3318  ls
 3319  cd 
 3320  cd opt/
 3321  cd hadoop-2.8.0/
 3322  ls
 3323  cd etc/hadoop/
 3324  sudo gedit hadoop-env.sh
 3325  cd ../..
 3326  pwd
 3327  jps
 3328  cd $HOME
 3329  hdfs dfs -mkdir /tmp
 3330  hdfs dfs -ls 
 3331  hdfs dfs -ls /
 3332  stop-all.sh
 3333  start-all.sh
 3334  jps
 3335  hdfs dfs -mkdir /usr
 3336  hdfs dfs -ls /
 3337  jps
 3338  stop-all.sh
 3339  ls
 3340  cd opt/hadoop-2.8.0/
 3341  ls
 3342  cd hdfs
 3343  ls
 3344  rm -rf datanode namenode
 3345  ls
 3346  cd $HOME
 3347  hdfs namenode -format
 3348  jps
 3349  start-all.sh
 3350  jps
 3351  hdfs dfs -ls /
 3352  hdfs dfs -mkdir /test
 3353  hdfs dfs -ls /
 3354  hdfs dfs -mkdir /tmp
 3355  hdfs dfs -ls /
 3356  hdfs dfs -chmod g+w /tmp
 3357  hdfs dfs -ls /
 3358  hdfs dfs -mkdir -p /$HIVE_HOME/warehouse
 3359  hdfs dfs -ls /
 3360  hdfs dfs -mkdir -p /user
 3361  hdfs dfs -ls /
 3362  hdfs dfs -mkdir /user/hive
 3363  hdfs dfs -ls /user
 3364  hdfs dfs -mkdir /usr/hive/warehouse
 3365  hsdf dfs -ls /usr/hiave
 3366  hfds dfs -ls /user/hive
 3367  hdfs dfs -ls /user/hive
 3368  hdfs dfs -ls /
 3369  hdfs dfs -ls /user
 3370  hdfs dfs -ls /user/hive
 3371  hdfs dfs -mkdir /user/hive/warehouse
 3372  hdfs dfs -ls /user/hiave
 3373  hdfs dfs -ls /user/hive
 3374  hdfs dfs -chmod g+w /user/hive/warehouse
 3375  hdfs dfs -ls /user/hive
 3376  cd opt/hive-2.3.5/conf
 3377  ls
 3378  sudo gedit hive-site.xml
 3379  cd ../bin
 3380  ls
 3381  history | grep "schematool"
 3382  schematool -initSchema -dbType derby
 3383  history > t.out
 3384  xdg-open t.out
 3385  hive --service metastore
 3386  cd ..
 3387  cd lib
 3388  ls gua*.*
 3389  mv guava-27.0-jre.jar guava-27.0-jre.jar.bak
 3390  cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar .
 3391  cd $HADOOP_HOME/share/hadoop/hdfs
 3392  cd lib
 3393  ls gua*.*
 3394  cp guava-11.0.2.jar \$HOME/opt/hive-2.3.5/lib
 3395  cp guava-11.0.2.jar $HOME
 3396  cd $HOME
 3397  ls gu*.*
 3398  mv guava-11.0.2.jar opt/hive-2.3.5/lib
 3399  cd opt/hive-2.3.5/lib
 3400  ls gua*.*
 3401  cd ../bin
 3402  schematool -initSchema -dbType derby
 3403  cd ..
 3404  ls
 3405  mv metastore_db metastore_db.bak
 3406  cd ..
 3407  ls
 3408  cd ..
 3409  ls
 3410  ls metastore_db/
 3411  mv metastore_db metastore_db.bak
 3412  cd opt/hive-2.3.5/bin
 3413  schematool -init -dbType derby
 3414  schematool -initSchema -dbType derby
 3415  hive
 3416  hive --service metastore
 3417  cd ..
 3418  ls
 3419  ls metastore_db
 3420  cd bin
 3421  hive --service metastore
 3422  cd ../lib
 3423  ls gua*.*
 3424  mv guava-11.0.2.jar guava-11.0.2.jar.bak
 3425  cd ..
 3426  cd bin
 3427  hive --service metastore
 3428  ls *.py
 3429  cat kafkawithtwitter.py 
 3430  grep "twitter" *.py
 3431  cd opt/flume-1.8.0/
 3432  ls
 3433  cd conf
 3434  ls
 3435  ls -l
 3436  xdg-open flumeTwitter2HDFS.properties
 3437  cd $HOME
 3438  ls *.py
 3439  sudo gedit twitter-kafka-consumer.py
 3440  history > tt.out
 3441  xdg-open tt.out
 3442  zookeeper-server-start.sh -daemon opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 3443  python3.8 consumer1.py
 3444  python3.8 producer1.py
 3445  ls *.py
 3446  cat kafka2spark-consumer.py 
 3447  python3.8 kafka2spark-producer.py 
 3448  ls *.py
 3449  more < twitter-streaming-test.py 
 3450  cat twitter-test.py
 3451  ls *.py
 3452  python3.8 twitter-streaming-test.py 
 3453  sudo gedit twitter-streaming-test.py 
 3454  python3.8 twitter-streaming-test.py 
 3455  python3.8 twitter-kafka-producer.py 
 3456  sudo gedit twitter-kafka-producer.py 
 3457  python3.8 twitter-kafka-producer.py 
 3458  python twitter-kafka-producer.py
 3459  jupyter-notebook
 3460  ls *.py
 3461  python3.8 read-api-reformat.py > t.out
 3462  xdg-open t.out
 3463  sudo gedit twitter-kafka-producer.py
 3464  jps
 3465  sudo gedit transformer.py
 3466  #
 3467  # twitter to kafka producer test script
 3468  #
 3469  from   kafka    import KafkaProducer
 3470  from   datetime import datetime
 3471  import tweepy
 3472  import sys
 3473  import re
 3474  TWEET_TOPICS = ['COVID-19']
 3475  KAFKA_BROKER = 'localhost:9092'
 3476  KAFKA_TOPIC = 'tweets'
 3477  class Streamer(tweepy.StreamListener):
 3478  def on_error(self, status_code):
 3479  if status_code == 402:;  False def on_status(self, status):
 3480  tweet= status.text
 3481  tweet= re.sub(r'RT\s@\w*:\s', '', tweet)
 3482  tweet= re.sub(r'https?.*', '', tweet)
 3483  global producer
 3484  producer.send(KAFKA_TOPIC, bytes(tweet, encoding='utf-8'))
 3485  print("TWEET>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
 3486  print(tweet)
 3487  print("<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<")
 3488  d= datetime.now()
 3489  #print(f'[{d.hour}:{d.minute}.{d.second}] sending tweet')
 3490  print("...sent tweet!")
 3491  # put your API keys here
 3492  consumer_key w84oZPOYFBhm79tqxpn"
 3493  consumer_secret_key = "WBBswNrbiknyJHT3k3wbNiBQ0p1NIidUZ0pnQAHlZ9SI6lSd63"
 3494  access_token access_token_secret = "SboVyteQ7NiKw4JYWnhbdb3LdSKSaml21dxkgDCadUOr7"
 3495  auth= tweepy.OAuthHandler(consumer_key, consumer_secret_key)
 3496  auth.set_access_token(access_token, access_token_secret)
 3497  api= tweepy.API(auth)
 3498  streamer= Streamer()
 3499  stream= tweepy.Stream(auth=api.auth, listener=streamer)
 3500  try:
 3501  producer = KafkaProducer(bootstrap_servers=KAFKA_BROKER)
 3502  except Exception as e:
 3503  #print(f'Error Connecting to Kafka --> {e}')
 3504  print("Error connecting to Kafka!")
 3505  sys.exit(0)
 3506  sys.exit(1)
 3507  stream.filter(track=TWEET_TOPICS)
 3508  spark-submit --jars spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar transformer.py
 3509  history | grep "spark-submit"
 3510  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 transformer.py
 3511  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 twitter-kafka-consumer.py
 3512  ls
 3513  ls metastore_db
 3514  ls 
 3515  cd metastore_db
 3516  ls
 3517  cd log
 3518  ls
 3519  cd $HOME
 3520  cd opt/hive-2.3.5/
 3521  ls
 3522  cd conf
 3523  ls
 3524  xdg-open hive-site.xml
 3525  more < hive-site.xml
 3526  cd ~/opt
 3527  cd spark-2.4.4/
 3528  ls
 3529  cd conf
 3530  ls
 3531  cd ..
 3532  ls
 3533  cd data
 3534  ls
 3535  cd ..
 3536  ls
 3537  cd /user
 3538  cd $HOME
 3539  cd /user
 3540  cd /
 3541  ls
 3542  mkdir user
 3543  sudo mkdir user
 3544  cd user
 3545  mkdir hive
 3546  sudo mkdir hive
 3547  cd hive
 3548  sudo mkdir warehouse
 3549  ls
 3550  cd warehouse
 3551  pwd
 3552  cd 
 3553  sudo chmod 777 user
 3554  cd /
 3555  sudo chmod 777 user
 3556  cd hive
 3557  cd /
 3558  cd /user
 3559  chmod 777 hive
 3560  sudo chmod /user
 3561  sudo chmod 777 /user
 3562  sudo chmod 777 /user/hive
 3563  sudo chmod 777 /user/hive/warehouse
 3564  cd ..
 3565  ls
 3566  ls user
 3567  lsl usr
 3568  ls usr
 3569  ls user
 3570  chmod 777 user
 3571  sudo chmod 777 user
 3572  sudo chmod 777 user/hive
 3573  sudo chmod 777 user/hive/warehouse
 3574  cd user
 3575  cd hive
 3576  cd warehouse
 3577  pwd
 3578  ls
 3579  cd $HOME
 3580  cd user
 3581  ls
 3582  cd /usr
 3583  ls
 3584  cd /user
 3585  ls
 3586  cd hive
 3587  ls
 3588  cd datahouse
 3589  cd warehouse
 3590  ls
 3591  pwd
 3592  ls *.py
 3593  cat test-covid-19-api.py
 3594  cat read-api-reformat.py
 3595  ls *.py
 3596  python3.8 read-api-reformat.py
 3597  hive
 3598  kafka-topics.sh --list --bootstrap-server localhost:9092
 3599  cd opt/kafka_2.12-2.0.0/
 3600  bin/kafka-topics.sh --list --bootstrap-server localhost:9092
 3601  bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic tweets
 3602  bin/kafka-topics.sh --list --bootstrap-server localhost:2181
 3603  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafkaTest
 3604  kafka-topics.sh --describe --zookeeper localhost:2181 --topic kafkaTest
 3605  bin/kafka-topics.sh --list --bootstrap-server localhost:2181
 3606  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic tweets
 3607  cat producer1.py
 3608  cd $HOME
 3609  cat producer1.py
 3610  kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic First_topic
 3611  kafka-topics.sh --list --bootstrap-server localhost:2181
 3612  cat consumer1.py
 3613  bin/kafka-topics.sh --list --zookeeper localhost:2181
 3614  kafka-topics.sh --list --zookeeper localhost:2181
 3615  python3.8 producer1.py
 3616  cat producer1.py
 3617  jps
 3618  python3.8 consumer1.py
 3619  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py 
 3620  sudo gedit kafka2spark-consumer.py 
 3621  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 kafka2spark-consumer.py 
 3622  ls *.py
 3623  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 twitter-kafka-consumer.py
 3624  ls *.py
 3625  cat producer1.py
 3626  ls *.py
 3627  cat twitter-kafka-producer.py
 3628  cat producer1.py
 3629  ls *.py
 3630  cat twitter-kafka-producer.
 3631  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 twitter-kafka-consumer.py
 3632  sudo gedit spark-hive-test.py
 3633  ls -l spark-hive-test.py
 3634  spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 spark-hive-test.py
 3635  pyspark
 3636  jps
 3637  cd opt/kafka_2.12-2.0.0/
 3638  ls
 3639  cd config
 3640  ls
 3641  more < server1.properties 
 3642  cd $HOME
 3643  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 3644  cd opt/kafka_2.12-2.0.0/config/
 3645  ls
 3646  xdg-open zookeeper.properties server1.properties 
 3647  xdg-open zookeeper.properties 
 3648  xdg-open server1.properties 
 3649  cd $HOME
 3650  kafka-server-start.sh server.properties 
 3651  jps
 3652  cd opt
 3653  cd kafka_2.12-2.0.0/config
 3654  ls
 3655  more < zookeeper.properties 
 3656  cd $HOME
 3657  zookeeper-server-start.sh -daemon opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 3658  cd opt
 3659  pwd
 3660  zookeeper-server-start.sh -daemon /home/field/kafka_2.12-2.0.0/config/zookeeper.properties 
 3661  zookeeper-server-start.sh -daemon /home/field/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 3662  cd $HOME
 3663  zookeeper-server-start.sh -daemon /home/field/opt/kafka_2.12-2.0.0/config/zookeeper.properties 
 3664  cd $KAFKA_HOME
 3665  ls
 3666  cd config
 3667  ls
 3668  zookeeper-server-start.sh -daemon zookeeper.properties 
 3669  cd ..
 3670  ls
 3671  cd ..config
 3672  cd config
 3673  ls
 3674  sudo gedit zookeeper.properties 
 3675  jps
 3676  zookeeper-server-start.sh -daemon zookeeper.properties 
 3677  sudo zookeeper-server-start.sh -daemon zookeeper.properties 
 3678  cd ..
 3679  ls
 3680  mkdir logs
 3681  sudo mkdir logs
 3682  ls
 3683  cd config
 3684  zookeeper-server-start.sh -daemon zookeeper.properties 
 3685  cd ../bin
 3686  ls
 3687  cd windows
 3688  ls
 3689  cd ..
 3690  ls
 3691  cd logs
 3692  ls
 3693  cd ..
 3694  chmod 777 logs
 3695  sudo chmod 777 logs
 3696  cd config
 3697  zookeeper-server-start.sh -daemon zookeeper.properties 
 3698  jps
 3699  cd $HOME
 3700  kafka-server-start.sh opt/kafka_2.12-2.0.0/config/server1.properties 
 3701  cd opt
 3702  ls
 3703  cd hadoop-2.8.0/
 3704  ls
 3705  cd etc
 3706  ls
 3707  cd hadoop
 3708  ls
 3709  cd ../..
 3710  ls
 3711  cd bin
 3712  ls
 3713  cd ..
 3714  ls
 3715  cd sbin
 3716  ls
 3717  cd $HOME
 3718  jps
 3719  start-all.sh 
 3720  jps
 3721  hdfs namenode -format
 3722  jps
 3723  start-dfs.sh
 3724  jps
 3725  start-dfs.sh
 3726  jps
 3727  hadoop fs -ls /
 3728  hive --version
 3729  ls /user/hive/warehouse
 3730  cd /user
 3731  cd /usr
 3732  ls
 3733  cd $HOME
 3734  ls
 3735  ls /d
 3736  ls -help
 3737  ls --help | more
 3738  ls -d
 3739  ls --directory
 3740  hadoop fs -mkdir -p /user/hive/warehouse
 3741  hdfs dfs -ls /
 3742  hdfs dfs -ls /user
 3743  hdfs dfs -ls /user/hive
 3744  hdfs dfs -ls /user/hive/warehouse
 3745  hadoop fs -mkdir -p /tmp
 3746  hadoop fs -chmod g+w /user/hive/warehouse
 3747  hadoop fs -chmod g+w /tmp
 3748  schematool -initSchema -dbType derby
 3749  ls
 3750  ls -d
 3751  cd opt/hive-2.3.5/
 3752  ls
 3753  cd metastore_db
 3754  ls
 3755  cd ..
 3756  mv metastore_db metastore_db.bak
 3757  sudo mv metastore_db metastore_db.bak
 3758  ls
 3759  cd metastore_db
 3760  ls
 3761  cd ..
 3762  cd metastore_db.bak
 3763  ls
 3764  cd metastore_db
 3765  ls
 3766  cd ..
 3767  sudo rm -rf metastore_db.bak
 3768  sudo mv metastore_db metastore_db.bak
 3769  ls
 3770  cd $HOME
 3771  schematool -initSchema -dbType derby
 3772  hive --services metastore
 3773  hive --service metastore
 3774  jps
 3775  hive --service metastore
 3776  ssh maria_dev@localhost -p 2222
 3777  ls *.out
 3778  history > hist003.out
